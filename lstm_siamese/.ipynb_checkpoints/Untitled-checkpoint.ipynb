{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "DuplicateFlagError",
     "evalue": "The flag 'is_char_based' is defined twice. First from C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py, Second from C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py.  Description from first occurrence: is character based syntactic similarity. if false then word embedding based semantic similarity is used.(default: True)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDuplicateFlagError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-20573b5520d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# ==================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m tf.flags.DEFINE_boolean(\"is_char_based\", False, \"is character based syntactic similarity. \"\n\u001b[0m\u001b[0;32m     20\u001b[0m                                                \u001b[1;34m\"if false then word embedding based semantic similarity is used.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                                                \"(default: True)\")\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\platform\\flags.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m           \u001b[1;34m'Use of the keyword argument names (flag_name, default_value, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m           'docstring) is deprecated, please use (name, default, help) instead.')\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\absl\\flags\\_defines.py\u001b[0m in \u001b[0;36mDEFINE_boolean\u001b[1;34m(name, default, help, flag_values, module_name, **args)\u001b[0m\n\u001b[0;32m    266\u001b[0m   \"\"\"\n\u001b[0;32m    267\u001b[0m   DEFINE_flag(_flag.BooleanFlag(name, default, help, **args),\n\u001b[1;32m--> 268\u001b[1;33m               flag_values, module_name)\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\absl\\flags\\_defines.py\u001b[0m in \u001b[0;36mDEFINE_flag\u001b[1;34m(flag, flag_values, module_name)\u001b[0m\n\u001b[0;32m    102\u001b[0m   \u001b[1;31m# Copying the reference to flag_values prevents pychecker warnings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m   \u001b[0mfv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m   \u001b[0mfv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m   \u001b[1;31m# Tell flag_values who's defining the flag.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\absl\\flags\\_flagvalues.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, name, flag)\u001b[0m\n\u001b[0;32m    425\u001b[0m         \u001b[1;31m# module is simply being imported a subsequent time.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDuplicateFlagError\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_flag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m     \u001b[0mshort_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshort_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[1;31m# If a new flag overrides an old one, we need to cleanup the old flag's\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDuplicateFlagError\u001b[0m: The flag 'is_char_based' is defined twice. First from C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py, Second from C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py.  Description from first occurrence: is character based syntactic similarity. if false then word embedding based semantic similarity is used.(default: True)"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "from input_helpers import InputHelper\n",
    "from siamese_network import SiameseLSTM\n",
    "from siamese_network_semantic import SiameseLSTMw2v\n",
    "from tensorflow.contrib import learn\n",
    "import gzip\n",
    "from random import random\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "tf.flags.DEFINE_boolean(\"is_char_based\", False, \"is character based syntactic similarity. \"\n",
    "                                               \"if false then word embedding based semantic similarity is used.\"\n",
    "                                               \"(default: True)\")\n",
    "\n",
    "tf.flags.DEFINE_string(\"word2vec_model\", \"wiki.simple.vec\", \"word2vec pre-trained embeddings file (default: None)\")\n",
    "tf.flags.DEFINE_string(\"word2vec_format\", \"text\", \"word2vec pre-trained embeddings file format (bin/text/textgz)(default: None)\")\n",
    "\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 300, \"Dimensionality of character embedding (default: 300)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 1.0, \"Dropout keep probability (default: 1.0)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularizaion lambda (default: 0.0)\")\n",
    "tf.flags.DEFINE_string(\"training_files\", \"train_snli.txt\", \"training file (default: None)\")  #for sentence semantic similarity use \"train_snli.txt\"\n",
    "tf.flags.DEFINE_integer(\"hidden_units\", 50, \"Number of hidden units (default:50)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 300, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 1000, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 1000, \"Save model after this many steps (default: 100)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS.flag_values_dict()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "if FLAGS.training_files==None:\n",
    "    print(\"Input Files List is empty. use --training_files argument.\")\n",
    "    exit()\n",
    "\n",
    "print(FLAGS.__flags)#运行结果见下图\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_document_length=15\n",
    "inpH = InputHelper()\n",
    "train_set, dev_set, vocab_processor,sum_no_of_batches = inpH.getDataSets(FLAGS.training_files,max_document_length, 10,\n",
    "                                                                         FLAGS.batch_size, FLAGS.is_char_based)\n",
    "trainableEmbeddings=False\n",
    "if FLAGS.is_char_based==True:\n",
    "    FLAGS.word2vec_model = False\n",
    "else:\n",
    "    if FLAGS.word2vec_model==None:\n",
    "        trainableEmbeddings=True\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\"\n",
    "          \"You are using word embedding based semantic similarity but \"\n",
    "          \"word2vec model path is empty. It is Recommended to use  --word2vec_model  argument. \"\n",
    "          \"Otherwise now the code is automatically trying to learn embedding values (may not help in accuracy)\"\n",
    "          \"\\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\")\n",
    "    else:\n",
    "        inpH.loadW2V(FLAGS.word2vec_model, FLAGS.word2vec_format)\n",
    "\n",
    "# Training\n",
    "# ==================================================\n",
    "print(\"starting graph def\")\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    print(\"started session\")\n",
    "    with sess.as_default():\n",
    "        if FLAGS.is_char_based:\n",
    "            siameseModel = SiameseLSTM(\n",
    "                sequence_length=max_document_length,\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=FLAGS.embedding_dim,\n",
    "                hidden_units=FLAGS.hidden_units,\n",
    "                l2_reg_lambda=FLAGS.l2_reg_lambda,\n",
    "                batch_size=FLAGS.batch_size\n",
    "            )\n",
    "        else:\n",
    "            siameseModel = SiameseLSTMw2v(\n",
    "                sequence_length=max_document_length,\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=FLAGS.embedding_dim,\n",
    "                hidden_units=FLAGS.hidden_units,\n",
    "                l2_reg_lambda=FLAGS.l2_reg_lambda,\n",
    "                batch_size=FLAGS.batch_size,\n",
    "                trainableEmbeddings=trainableEmbeddings\n",
    "            )\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        print(\"initialized siameseModel object\")\n",
    "    \n",
    "    grads_and_vars=optimizer.compute_gradients(siameseModel.loss)\n",
    "    tr_op_set = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "    print(\"defined training_ops\")\n",
    "    # Keep track of gradient values and sparsity (optional)\n",
    "    grad_summaries = []\n",
    "    for g, v in grads_and_vars:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "    print(\"defined gradient summaries\")\n",
    "    # Output directory for models and summaries\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", siameseModel.loss)\n",
    "    acc_summary = tf.summary.scalar(\"accuracy\", siameseModel.accuracy)\n",
    "\n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "    # Dev summaries\n",
    "    dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "    dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "    dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "    # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=100)\n",
    "\n",
    "    # Write vocabulary\n",
    "    vocab_processor.save(os.path.join(checkpoint_dir, \"vocab\"))\n",
    "\n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print(\"init all variables\")\n",
    "    graph_def = tf.get_default_graph().as_graph_def()\n",
    "    graphpb_txt = str(graph_def)\n",
    "    with open(os.path.join(checkpoint_dir, \"graphpb.txt\"), 'w') as f:\n",
    "        f.write(graphpb_txt)\n",
    "\n",
    "    if FLAGS.word2vec_model :\n",
    "        # initial matrix with random uniform\n",
    "        initW = np.random.uniform(-0.25,0.25,(len(vocab_processor.vocabulary_), FLAGS.embedding_dim))\n",
    "        #initW = np.zeros(shape=(len(vocab_processor.vocabulary_), FLAGS.embedding_dim))\n",
    "        # load any vectors from the word2vec\n",
    "        print(\"initializing initW with pre-trained word2vec embeddings\")\n",
    "        for w in vocab_processor.vocabulary_._mapping:\n",
    "            arr=[]\n",
    "            s = re.sub('[^0-9a-zA-Z]+', '', w)\n",
    "            if w in inpH.pre_emb:\n",
    "                arr=inpH.pre_emb[w]\n",
    "            elif w.lower() in inpH.pre_emb:\n",
    "                arr=inpH.pre_emb[w.lower()]\n",
    "            elif s in inpH.pre_emb:\n",
    "                arr=inpH.pre_emb[s]\n",
    "            elif s.isdigit():\n",
    "                arr=inpH.pre_emb[\"zero\"]\n",
    "            if len(arr)>0:\n",
    "                idx = vocab_processor.vocabulary_.get(w)\n",
    "                initW[idx]=np.asarray(arr).astype(np.float32)\n",
    "        print(\"Done assigning intiW. len=\"+str(len(initW)))\n",
    "        inpH.deletePreEmb()\n",
    "        gc.collect()\n",
    "        sess.run(siameseModel.W.assign(initW))\n",
    "\n",
    "    def train_step(x1_batch, x2_batch, y_batch):\n",
    "        \"\"\"\n",
    "        A single training step\n",
    "        \"\"\"\n",
    "        if random()>0.5:\n",
    "            feed_dict = {\n",
    "                siameseModel.input_x1: x1_batch,\n",
    "                siameseModel.input_x2: x2_batch,\n",
    "                siameseModel.input_y: y_batch,\n",
    "                siameseModel.dropout_keep_prob: FLAGS.dropout_keep_prob,\n",
    "            }\n",
    "        else:\n",
    "            feed_dict = {\n",
    "                siameseModel.input_x1: x2_batch,\n",
    "                siameseModel.input_x2: x1_batch,\n",
    "                siameseModel.input_y: y_batch,\n",
    "                siameseModel.dropout_keep_prob: FLAGS.dropout_keep_prob,\n",
    "            }\n",
    "        _, step, loss, accuracy, dist, sim, summaries = sess.run([tr_op_set, global_step, siameseModel.loss, siameseModel.accuracy, siameseModel.distance, siameseModel.temp_sim, train_summary_op],  feed_dict)\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"TRAIN {}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "        train_summary_writer.add_summary(summaries, step)\n",
    "        print(y_batch, dist, sim)\n",
    "\n",
    "    def dev_step(x1_batch, x2_batch, y_batch):\n",
    "        \"\"\"\n",
    "        A single training step\n",
    "        \"\"\" \n",
    "        if random()>0.5:\n",
    "            feed_dict = {\n",
    "                siameseModel.input_x1: x1_batch,\n",
    "                siameseModel.input_x2: x2_batch,\n",
    "                siameseModel.input_y: y_batch,\n",
    "                siameseModel.dropout_keep_prob: 1.0,\n",
    "            }\n",
    "        else:\n",
    "            feed_dict = {\n",
    "                siameseModel.input_x1: x2_batch,\n",
    "                siameseModel.input_x2: x1_batch,\n",
    "                siameseModel.input_y: y_batch,\n",
    "                siameseModel.dropout_keep_prob: 1.0,\n",
    "            }\n",
    "        step, loss, accuracy, sim, summaries = sess.run([global_step, siameseModel.loss, siameseModel.accuracy, siameseModel.temp_sim, dev_summary_op],  feed_dict)\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"DEV {}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "        dev_summary_writer.add_summary(summaries, step)\n",
    "        print (y_batch, sim)\n",
    "        return accuracy\n",
    "\n",
    "    # Generate batches\n",
    "    batches=inpH.batch_iter(\n",
    "                list(zip(train_set[0], train_set[1], train_set[2])), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "\n",
    "    ptr=0\n",
    "    max_validation_acc=0.0\n",
    "    for nn in xrange(sum_no_of_batches*FLAGS.num_epochs):\n",
    "        batch = batches.next()\n",
    "        if len(batch)<1:\n",
    "            continue\n",
    "        x1_batch,x2_batch, y_batch = zip(*batch)\n",
    "        if len(y_batch)<1:\n",
    "            continue\n",
    "        train_step(x1_batch, x2_batch, y_batch)\n",
    "        current_step = tf.train.global_step(sess, global_step)\n",
    "        sum_acc=0.0\n",
    "        if current_step % FLAGS.evaluate_every == 0:\n",
    "            print(\"\\nEvaluation:\")\n",
    "            dev_batches = inpH.batch_iter(list(zip(dev_set[0],dev_set[1],dev_set[2])), FLAGS.batch_size, 1)\n",
    "            for db in dev_batches:\n",
    "                if len(db)<1:\n",
    "                    continue\n",
    "                x1_dev_b,x2_dev_b,y_dev_b = zip(*db)\n",
    "                if len(y_dev_b)<1:\n",
    "                    continue\n",
    "                acc = dev_step(x1_dev_b, x2_dev_b, y_dev_b)\n",
    "                sum_acc = sum_acc + acc\n",
    "            print(\"\")\n",
    "        if current_step % FLAGS.checkpoint_every == 0:\n",
    "            if sum_acc >= max_validation_acc:\n",
    "                max_validation_acc = sum_acc\n",
    "                saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                tf.train.write_graph(sess.graph.as_graph_def(), checkpoint_prefix, \"graph\"+str(nn)+\".pb\", as_text=False)\n",
    "                print(\"Saved model {} with sum_accuracy={} checkpoint to {}\\n\".format(nn, max_validation_acc, checkpoint_prefix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_char_based': <absl.flags._flag.BooleanFlag object at 0x0000004E2B9C47B8>, 'word2vec_model': <absl.flags._flag.Flag object at 0x0000004E2BF35710>, 'word2vec_format': <absl.flags._flag.Flag object at 0x0000004E2BF35668>, 'embedding_dim': <absl.flags._flag.Flag object at 0x0000004E2BF359B0>, 'dropout_keep_prob': <absl.flags._flag.Flag object at 0x0000004E2BF359E8>, 'l2_reg_lambda': <absl.flags._flag.Flag object at 0x0000004E2BF358D0>, 'training_files': <absl.flags._flag.Flag object at 0x0000004E2BF35A58>, 'hidden_units': <absl.flags._flag.Flag object at 0x0000004E2BF35AC8>, 'batch_size': <absl.flags._flag.Flag object at 0x0000004E2BF35B70>, 'num_epochs': <absl.flags._flag.Flag object at 0x0000004E2BF35BE0>, 'evaluate_every': <absl.flags._flag.Flag object at 0x0000004E2BF35C88>, 'checkpoint_every': <absl.flags._flag.Flag object at 0x0000004E2BF35D30>, 'allow_soft_placement': <absl.flags._flag.BooleanFlag object at 0x0000004E2BF35EB8>, 'log_device_placement': <absl.flags._flag.BooleanFlag object at 0x0000004E2BF35F60>, 'is_chadr_based': <absl.flags._flag.BooleanFlag object at 0x0000004E2BEDDC18>, 'para_name_1': <absl.flags._flag.Flag object at 0x0000004E11A74828>, 'para_name2_1': <absl.flags._flag.Flag object at 0x0000004E11A69400>, 'batch_siz1e': <absl.flags._flag.Flag object at 0x0000004E11A693C8>, 'num_epochs1': <absl.flags._flag.Flag object at 0x0000004E11A69470>}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#调用flags内部的DEFINE_string函数来制定解析规则\n",
    "\n",
    "#FLAGS是一个对象，保存了解析后的命令行参数\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS.flag_values_dict()#进行解析，加上这一句可以把FLAGS.__flags变成一个字典\n",
    "\n",
    "print(FLAGS.__flags)#运行结果见下图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
