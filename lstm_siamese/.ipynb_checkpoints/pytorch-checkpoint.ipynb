{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- encoding: utf-8 -*-\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "#from utils.parameter_initialization import xavier_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from train_snli.txt\n",
      "Building vocabulary\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\deep-siamese-text-similarity-master\\preprocess.py:34: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "Length of loaded vocabulary =31337\n",
      "dumping validation 0\n",
      "Train/Dev split for train_snli.txt: 330635/36738\n",
      "Loading W2V data...\n",
      "loaded word2vec len  111051\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "from input_helpers import InputHelper\n",
    "from tensorflow.contrib import learn\n",
    "import gzip\n",
    "from random import random\n",
    "import pickle\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "is_char_based=False\n",
    "word2vec_model=\"D:\\simple_vec\\wiki.simple.bin\"\n",
    "word2vec_format=\"bin\"\n",
    "embedding_dim=300\n",
    "dropout_keep_prob=1.0\n",
    "l2_reg_lambda=0.0\n",
    "hidden_units=50\n",
    "\n",
    "# Training parameters\n",
    "batch_size=64\n",
    "num_epochs=300\n",
    "evaluate_every=1000\n",
    "checkpoint_every=1000\n",
    "\n",
    "# Misc Parameters\n",
    "allow_soft_placement=True\n",
    "log_device_placement=False\n",
    "trainableEmbeddings=False\n",
    "\n",
    "training_files=\"train_snli.txt\"\n",
    "\n",
    "max_document_length=15\n",
    "inpH = InputHelper()\n",
    "train_set, dev_set, vocab_processor,sum_no_of_batches = inpH.getDataSets(training_files, 10,max_document_length,batch_size, is_char_based)\n",
    "trainableEmbeddings=False\n",
    "if is_char_based==True:\n",
    "    word2vec_model = False\n",
    "\n",
    "inpH.loadW2V(word2vec_model, word2vec_format)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31337"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size=len(vocab_processor.vocabulary_)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing initW with pre-trained word2vec embeddings\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def _calculate_fan_in_and_fan_out(tensor):\n",
    "    if tensor.ndimension() < 2:\n",
    "        raise ValueError(\"fan in and fan out can not be computed for tensor of size \", tensor.size())\n",
    "\n",
    "    if tensor.ndimension() == 2:  # Linear\n",
    "        fan_in = tensor.size(1)\n",
    "        fan_out = tensor.size(0)\n",
    "    else:\n",
    "        num_input_fmaps = tensor.size(1)\n",
    "        num_output_fmaps = tensor.size(0)\n",
    "        receptive_field_size = np.prod(tensor.numpy().shape[2:])\n",
    "        fan_in = num_input_fmaps * receptive_field_size\n",
    "        fan_out = num_output_fmaps * receptive_field_size\n",
    "\n",
    "    return fan_in, fan_out\n",
    "\n",
    "def xavier_normal(tensor, gain=1):\n",
    "    \"\"\"Fills the input Tensor or Variable with values according to the method described in \"Understanding the difficulty of training\n",
    "       deep feedforward neural networks\" - Glorot, X. and Bengio, Y., using a normal distribution.\n",
    "       The resulting tensor will have values sampled from normal distribution with mean=0 and\n",
    "       std = gain * sqrt(2/(fan_in + fan_out))\n",
    "    Args:\n",
    "        tensor: a n-dimension torch.Tensor\n",
    "        gain: an optional scaling factor to be applied\n",
    "    Examples:\n",
    "        w = torch.Tensor(3, 5)\n",
    "        xavier_normal(w, gain=np.sqrt(2.0))\n",
    "    \"\"\"\n",
    "\n",
    "    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n",
    "    std = gain * np.sqrt(2.0 / (fan_in + fan_out))\n",
    "    return tensor.normal_(0, std)\n",
    "\n",
    "\n",
    "initW = xavier_normal(torch.randn([vocab_size, 300])).numpy()\n",
    "\n",
    "\n",
    "# initial matrix with random uniform\n",
    "\n",
    "print(\"initializing initW with pre-trained word2vec embeddings\")\n",
    "\n",
    "\n",
    "for w in vocab_processor.vocabulary_._mapping:\n",
    "    arr=[]\n",
    "    s = re.sub('[^0-9a-zA-Z]+', '', w)\n",
    "    if w in inpH.pre_emb:\n",
    "        arr=inpH.pre_emb[w]\n",
    "    elif w.lower() in inpH.pre_emb:\n",
    "        arr=inpH.pre_emb[w.lower()]\n",
    "    elif s in inpH.pre_emb:\n",
    "        arr=inpH.pre_emb[s]\n",
    "    elif s.isdigit():\n",
    "        arr=inpH.pre_emb[\"zero\"]\n",
    "    if len(arr)>0:\n",
    "        idx = vocab_processor.vocabulary_.get(w)\n",
    "        initW[idx]=np.asarray(arr).astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(batches):\n",
    "    batch=batches.__next__()\n",
    "    x1_batch,x2_batch, y_batch = zip(*batch)\n",
    "    x1_batch=torch.tensor(x1_batch)\n",
    "    x2_batch=torch.tensor(x2_batch)\n",
    "    y_batch=np.array(y_batch)\n",
    "    y_batch=torch.tensor(y_batch).view(-1)\n",
    "    return x1_batch,x2_batch,y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size,embedding_dims,hidden_dims,num_layers,batch_size,dropout_p,is_train):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.hidden_dims=hidden_dims\n",
    "        self.embedding_dims=embedding_dims\n",
    "        self.num_layers=num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.is_train=is_train\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        \n",
    "        self.embedding_table = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_dims,padding_idx=0, max_norm=None, \n",
    "                                       scale_grad_by_freq=False, sparse=False)\n",
    "       # print(self.embedding_table.weight)\n",
    "        self.embedding_table.weight.requires_grad=self.is_train\n",
    "        #print(self.embedding_table.weight.requires_grad)\n",
    "        self.lstm_rnn = nn.LSTM(input_size=self.embedding_dims,hidden_size=self.hidden_dims, num_layers=1)\n",
    "     #   print(lstm_rnn..requires_grad)\n",
    "        self.dropout = nn.Dropout(self.dropout_p )\n",
    "        \n",
    "    def initHidden(self):\n",
    "        hidden_a = torch.randn(1, self.batch_size,self.hidden_dims)\n",
    "        hidden_b = torch.randn(1,self.batch_size,self.hidden_dims)\n",
    "        return (hidden_a,hidden_b)\n",
    "        \n",
    "    def forward(self,data,batch_size,hidden):\n",
    "        output = self.embedding_table(data).view(-1,batch_size,embedding_dims)\n",
    "\n",
    "        output, hidden= self.lstm_rnn(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "\n",
    "#a.embedding_table.weight\n",
    "#dir(a.lstm_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "class SiameseClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size,embedding_dims,hidden_dims,num_layers,batch_size,dropout_p,pretrained_embeddings,is_train,learning_rate):\n",
    "        super(SiameseClassifier, self).__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.hidden_dims=hidden_dims\n",
    "        self.embedding_dims=embedding_dims\n",
    "        self.num_layers=num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.is_train=is_train\n",
    "        self.learning_rate=learning_rate\n",
    "        self.batch_size=batch_size\n",
    " \n",
    "        \n",
    "        self.encoder_a = self.encoder_b = LSTMEncoder(self.vocab_size,self.embedding_dims,self.hidden_dims,self.num_layers,self.batch_size,self.dropout_p,self.is_train)\n",
    "        # Initialize pre-trained embeddings, if given\n",
    "        self.initialize_parameters()\n",
    "        \n",
    "        if not self.is_train:\n",
    "              self.encoder_a.embedding_table.weight.data.copy_(pretrained_embeddings)\n",
    "        \n",
    "        self.pretrained_embeddings=self.encoder_a.embedding_table.weight\n",
    "\n",
    "        # Initialize network parameters\n",
    "      \n",
    "\n",
    "        # Initialize network optimizers\n",
    "        self.optimizer_a = optim.Adam(filter(lambda p: p.requires_grad, self.encoder_a.parameters()), lr=self.learning_rate,\n",
    "                                      betas=(0.25, 0.999))\n",
    "        self.optimizer_b = optim.Adam(filter(lambda p: p.requires_grad,self.encoder_b.parameters()), lr=self.learning_rate,betas=(0.25, 0.999))\n",
    "        \n",
    "        print(\"Model Compile\")\n",
    "    \n",
    "    def forward(self,x1,x2,y):\n",
    "        \"\"\" Performs a single forward pass through the siamese architecture. \"\"\"\n",
    "        # Checkpoint the encoder state\n",
    "        self.x1=x1\n",
    "        self.x2=x2\n",
    "        self.y=y\n",
    "        state_dict = self.encoder_a.state_dict()\n",
    "\n",
    "        # Obtain sentence encodings from each encoder\n",
    "        hidden_a= self.encoder_a.initHidden()\n",
    "        \n",
    "        output_a,hidden_a=self.encoder_b(self.x1,self.batch_size,hidden_a)\n",
    "            \n",
    "\n",
    "        # Restore checkpoint to establish weight-sharing\n",
    "        self.encoder_b.load_state_dict(state_dict)\n",
    "        hidden_b= self.encoder_b.initHidden()\n",
    "        \n",
    "        output_b,hidden_b=self.encoder_b(self.x2,self.batch_size,hidden_b)\n",
    "        \n",
    "        encoding_a=hidden_a[0]\n",
    "        encoding_b=hidden_b[0] \n",
    "       \n",
    "\n",
    "        # Format sentence encodings as 2D tensors\n",
    "        self.encoding_a = encoding_a.squeeze(dim=0)  #[batch, hidden]\n",
    "        self.encoding_b = encoding_b.squeeze(dim=0)\n",
    " \n",
    "\n",
    "    def get_loss(self):\n",
    "        y=self.y.float()\n",
    "        dist_sq =torch.sqrt(torch.sum(torch.pow(self.encoding_a-self.encoding_b , 2), 1))\n",
    "        dist_1=torch.sqrt(torch.sum(torch.pow(self.encoding_a,2),1))\n",
    "        dist_2=torch.sqrt(torch.sum(torch.pow(self.encoding_b,2),1))\n",
    "        dist=torch.div(dist_sq,dist_1+dist_2)\n",
    "        self.dist=dist\n",
    "        loss=y*(torch.pow(dist,2))+(1-y)*torch.clamp(1-dist, min=0.0)\n",
    "        loss=torch.sum(loss)/2.0/dist.size()[0]\n",
    "        \n",
    "        self.loss=loss\n",
    "    def get_accuracy(self):\n",
    "        a=np.rint(self.dist.data.numpy())\n",
    "        b=np.ones(self.batch_size)\n",
    "        c=np.equal(b-a, self.y.data.numpy())\n",
    "        g=[1 if x else 0 for x in c ]\n",
    "        accu=np.sum(g)/batch_size\n",
    "        self.accuracy=accu\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\" Initializes network parameters. \"\"\"\n",
    "        state_dict = self.encoder_a.state_dict()\n",
    "        for key in state_dict.keys():\n",
    "            if '.weight' in key:\n",
    "                state_dict[key] = xavier_normal(state_dict[key])\n",
    "            if '.bias' in key:\n",
    "                bias_length = state_dict[key].size()[0]\n",
    "                start, end = bias_length // 4, bias_length // 2\n",
    "                state_dict[key][start:end].fill_(2.5)\n",
    "        self.encoder_a.load_state_dict(state_dict)\n",
    "    def train_step(self, train_batch_a, train_batch_b, train_labels):\n",
    "\n",
    "        # Get gradients\n",
    "        self.encoder_a.zero_grad()  \n",
    "        self.forward(train_batch_a, train_batch_b, train_labels)\n",
    "       # encoder_a == encoder_b\n",
    "        self.get_loss()\n",
    "        self.get_accuracy()\n",
    "        self.loss.backward()\n",
    "\n",
    "        # Clip gradients\n",
    "        clip_grad_norm(filter(lambda p: p.requires_grad,self.encoder_b.parameters()), 0.25)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer_a.step()\n",
    "    def dev_step(self, train_batch_a, train_batch_b, train_labels):\n",
    "\n",
    "        # Get gradients\n",
    "        self.encoder_a.zero_grad()  \n",
    "        self.forward(train_batch_a, train_batch_b, train_labels)\n",
    "       # encoder_a == encoder_b\n",
    "        self.get_loss()\n",
    "        self.get_accuracy()\n",
    "        #self.loss.backward()\n",
    "\n",
    "        # Clip gradients\n",
    "        #clip_grad_norm(filter(lambda p: p.requires_grad,self.encoder_b.parameters()), 0.25)\n",
    "        \n",
    "        # Optimize\n",
    "       # self.optimizer_a.step()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Compile\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from utils.init_and_storage import add_pretrained_embeddings, extend_embeddings, update_learning_rate, save_network\n",
    "\n",
    "import os\n",
    "\n",
    "save_dir = 'D:/models/slstm'\n",
    "pretraining_dir = 'D:/models/slstm'\n",
    "\n",
    "start_early_stopping=90\n",
    "start_annealing=3\n",
    "save_freq=5\n",
    "best_validation_accuracy=0\n",
    "patience=3\n",
    "epochs_without_improvement=0\n",
    "\n",
    "batch_size=32\n",
    "num_epochs=100\n",
    "\n",
    "vocab_size=31337\n",
    "hidden_dims=50\n",
    "embedding_dims=300\n",
    "num_layers=3\n",
    "dropout_p = 1.\n",
    "learning_rate=0.0001\n",
    "old_learning_rate=0.0001\n",
    "\n",
    "is_train=False\n",
    "\n",
    "batch_size=32\n",
    "pretrained_embeddings=torch.tensor(initW)\n",
    "report_freq=200\n",
    "final_epoch=0 \n",
    "model=SiameseClassifier(vocab_size,embedding_dims,hidden_dims,num_layers,batch_size,dropout_p,pretrained_embeddings,is_train,learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:106: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Training Batch: 200 | Average loss since batch 0: 0.1926 | Model accuracy: 0.5097 \n",
      "Average training batch loss at epoch 0: 0.1923  batch accuracy:  0.5048\n",
      "Epoch: 2 | Training Batch: 200 | Average loss since batch 0: 0.1906 | Model accuracy: 0.5039 \n",
      "Average training batch loss at epoch 1: 0.1905  batch accuracy:  0.5003\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-162-8610e227e439>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mtrain_batch_loss\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-144-f445f1de9db0>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, train_batch_a, train_batch_b, train_labels)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;31m# Get gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder_a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_batch_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_batch_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m        \u001b[1;31m# encoder_a == encoder_b\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-144-f445f1de9db0>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x1, x2, y)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# Restore checkpoint to establish weight-sharing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder_b\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mhidden_b\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder_b\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitHidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m    704\u001b[0m                     \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m         \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(module, prefix)\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m                     \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m         \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(module, prefix)\u001b[0m\n\u001b[0;32m    699\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m             module._load_from_state_dict(\n\u001b[1;32m--> 701\u001b[1;33m                 state_dict, prefix, strict, missing_keys, unexpected_keys, error_msgs)\n\u001b[0m\u001b[0;32m    702\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_load_from_state_dict\u001b[1;34m(self, state_dict, prefix, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[0;32m    657\u001b[0m                     \u001b[0minput_param\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m                     \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m                     error_msgs.append('While copying the parameter named \"{}\", '\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#subset=10000\n",
    "subset=train_set[0].shape[0]+1\n",
    "for epoch in range(num_epochs):\n",
    "    batches=inpH.batch_iter(list(zip(train_set[0][0:subset], train_set[1][0:subset], train_set[2][0:subset])), batch_size, 1)\n",
    "    running_loss = list()\n",
    "    total_train_loss = list()\n",
    "    running_accu=list()\n",
    "    total_accu=list()\n",
    "    for i in range(int(train_set[0][0:subset].shape[0]/batch_size)):\n",
    "        \n",
    "        x1,x2,y=sample(batches)\n",
    "        model.train_step(x1,x2,y)\n",
    "        train_batch_loss =model.loss.data[0]\n",
    "    \n",
    "        running_loss.append(train_batch_loss)\n",
    "        total_train_loss.append(train_batch_loss)\n",
    "        batch_accu=model.accuracy\n",
    "        running_accu.append(batch_accu)\n",
    "        total_accu.append(batch_accu)\n",
    "        if((i+1)%report_freq==0):\n",
    "            running_avg_loss = sum(running_loss) / len(running_loss)\n",
    "            running_avg_accu=sum(running_accu)/len(running_accu)\n",
    "            print('Epoch: %d | Training Batch: %d | Average loss since batch %d: %.4f | Model accuracy: %.4f ' %\n",
    "                  (epoch+1, i+1, i - report_freq+1, running_avg_loss,running_avg_accu))\n",
    "            running_loss = list()\n",
    "            running_accu=list()\n",
    "    avg_training_loss = sum(total_train_loss) / len(total_train_loss)\n",
    "    avg_training_accuracy = sum(total_accu) / len(total_accu)\n",
    "    print('Average training batch loss at epoch %d: %.4f  batch accuracy:  %.4f' % (epoch, avg_training_loss, avg_training_accuracy))\n",
    "\n",
    "     # Validate after each epoch; set tracking variables\n",
    "    if epoch >= start_early_stopping:\n",
    "        total_valid_loss = list()\n",
    "        # Initiate the training data loader\n",
    "        batches=inpH.batch_iter(list(zip(dev_set[0], dev_set[1], dev_set[2])), batch_size, 1)\n",
    "\n",
    "        # Validation loop (i.e. perform inference on the validation set)\n",
    "        for i in range(int(dev_set[0].shape[0]/batch_size)):\n",
    "            x1,x2,y=sample(batches)\n",
    "            # Get predictions and update tracking values\n",
    "            model.dev_step(x1, x2, y)\n",
    "            valid_batch_loss = model.loss.data[0]\n",
    "            total_valid_loss.append(valid_batch_loss)\n",
    "\n",
    "        # Report fold statistics\n",
    "        avg_valid_accuracy = sum(total_valid_loss) / len(total_valid_loss)\n",
    "        print('Average validation fold accuracy at epoch %d: %.4f' % (epoch, avg_valid_accuracy))\n",
    "        # Save network parameters if performance has improved\n",
    "        if avg_valid_accuracy <= best_validation_accuracy:\n",
    "            epochs_without_improvement += 1\n",
    "        else:\n",
    "            best_validation_accuracy = avg_valid_accuracy\n",
    "            epochs_without_improvement = 0\n",
    "            \n",
    "    # Save network parameters at the end of each nth epoch\n",
    "    if epoch % save_freq == 0 and epoch != 0:\n",
    "        print('Saving model networks after completing epoch %d' % epoch)\n",
    "        save_network(model.encoder_a, 'sim_classifier', epoch, save_dir)\n",
    "\n",
    "    # Anneal learning rate:\n",
    "    if epochs_without_improvement == start_annealing:\n",
    "        old_learning_rate = model.learning_rate\n",
    "        learning_rate *= annealing_factor\n",
    "        update_learning_rate(model.optimizer_a, learning_rate)\n",
    "        print('Learning rate has been updated from %.4f to %.4f' % (old_learning_rate, learning_rate))\n",
    "\n",
    "    # Terminate training early, if no improvement has been observed for n epochs\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print('Stopping training early after %d epochs, following %d epochs without performance improvement.' %\n",
    "              (epoch, epochs_without_improvement))\n",
    "        final_epoch = epoch\n",
    "        save_network(model.encoder_a, 'sim_classifier', 'latest', save_dir)\n",
    "        break\n",
    "print(\"Done\")\n",
    "\n",
    "\n",
    "\n",
    "print('Training procedure concluded after %d epochs total. Best validated epoch: %d.'\n",
    "      % (final_epoch, final_epoch - patience))\n",
    "if model.is_train:\n",
    "    # Save pretrained embeddings and the vocab object\n",
    "    pretrained_path = os.path.join(save_dir, 'pretrained.pkl')\n",
    "    pretrained_embeddings = model.pretrained_embeddings.data\n",
    "    with open(pretrained_path, 'wb') as f:\n",
    "        pickle.dump((pretrained_embeddings), f)\n",
    "    print('Pre-trained parameters saved to %s' % pretrained_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained parameters saved to D:/models/slstm\\pretrained.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Save pretrained embeddings and the vocab object\n",
    "pretrained_path = os.path.join(save_dir, 'pretrained.pkl')\n",
    "pretrained_embeddings = model.pretrained_embeddings.data\n",
    "with open(pretrained_path, 'wb') as f:\n",
    "    pickle.dump((pretrained_embeddings), f)\n",
    "print('Pre-trained parameters saved to %s' % pretrained_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\pc\\\\deep-siamese-text-similarity-master'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(model.pre)\n",
    "if opt.pre_training:\n",
    "    # Save pretrained embeddings and the vocab object\n",
    "    pretrained_path = os.path.join(save_dir, 'pretrained.pkl')\n",
    "    pretrained_embeddings = classifier.encoder_a.embedding_table.weight.data\n",
    "    with open(pretrained_path, 'wb') as f:\n",
    "        pickle.dump((pretrained_embeddings, vocab), f)\n",
    "    print('Pre-trained parameters saved to %s' % pretrained_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\pc\\\\deep-siamese-text-similarity-master\\\\model'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.path.join(a,'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = os.path.join(os.path.dirname(__file__), '..')\n",
    "\n",
    "save_dir = os.path.join(self.home_dir, 'similarity_estimator/models')\n",
    "        self.pretraining_dir = os.path.join(self.save_dir, 'pretraining')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.6235,  3.6869,  3.2830,  3.2962,  3.7055,  3.9573,  3.5993,\n",
       "         2.9149,  3.2193,  3.7871,  3.6147,  3.0583,  3.4577,  4.0743,\n",
       "         2.4509,  3.4638,  3.2637,  2.9339,  3.4353,  3.3313,  3.5914,\n",
       "         3.8139,  3.0272,  4.3405,  3.4652,  3.4254,  2.9277,  3.0703,\n",
       "         4.4681,  3.6014,  3.6133,  3.0554])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs=10\n",
    "for epoch in range(num_epochs):\n",
    "    # Declare tracking variables\n",
    "    running_loss = list()\n",
    "    total_train_loss = list()\n",
    "\n",
    "    # Initiate the training data loader\n",
    "    train_loader = DataServer([train_data, train_labels], vocab, opt, is_train=True, use_buckets=True, volatile=False)\n",
    "\n",
    "    # Training loop\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # Obtain data\n",
    "        s1_var, s2_var, label_var = data\n",
    "        classifier.train_step(s1_var, s2_var, label_var)\n",
    "        train_batch_loss = classifier.loss.data[0]\n",
    "\n",
    "        running_loss.append(train_batch_loss)\n",
    "        total_train_loss.append(train_batch_loss)\n",
    "\n",
    "        if i % opt.report_freq == 0 and i != 0:\n",
    "            running_avg_loss = sum(running_loss) / len(running_loss)\n",
    "            print('Epoch: %d | Training Batch: %d | Average loss since batch %d: %.4f' %\n",
    "                  (epoch, i, i - opt.report_freq, running_avg_loss))\n",
    "            running_loss = list()\n",
    "\n",
    "    # Report epoch statistics\n",
    "    avg_training_accuracy = sum(total_train_loss) / len(total_train_loss)\n",
    "    print('Average training batch loss at epoch %d: %.4f' % (epoch, avg_training_accuracy))\n",
    "\n",
    "    # Validate after each epoch; set tracking variables\n",
    "    if epoch >= opt.start_early_stopping:\n",
    "        total_valid_loss = list()\n",
    "\n",
    "        # Initiate the training data loader\n",
    "        valid_loader = DataServer([valid_data, valid_labels], vocab, opt, is_train=True, use_buckets=False,\n",
    "                                  volatile=True)\n",
    "\n",
    "        # Validation loop (i.e. perform inference on the validation set)\n",
    "        for i, data in enumerate(valid_loader):\n",
    "            s1_var, s2_var, label_var = data\n",
    "            # Get predictions and update tracking values\n",
    "            classifier.test_step(s1_var, s2_var, label_var)\n",
    "            valid_batch_loss = classifier.loss.data[0]\n",
    "            total_valid_loss.append(valid_batch_loss)\n",
    "\n",
    "        # Report fold statistics\n",
    "        avg_valid_accuracy = sum(total_valid_loss) / len(total_valid_loss)\n",
    "        print('Average validation fold accuracy at epoch %d: %.4f' % (epoch, avg_valid_accuracy))\n",
    "        # Save network parameters if performance has improved\n",
    "        if avg_valid_accuracy <= best_validation_accuracy:\n",
    "            epochs_without_improvement += 1\n",
    "        else:\n",
    "            best_validation_accuracy = avg_valid_accuracy\n",
    "            epochs_without_improvement = 0\n",
    "            save_network(classifier.encoder_a, 'sim_classifier', 'latest', save_dir)\n",
    "\n",
    "    # Save network parameters at the end of each nth epoch\n",
    "    if epoch % opt.save_freq == 0 and epoch != 0:\n",
    "        print('Saving model networks after completing epoch %d' % epoch)\n",
    "        save_network(classifier.encoder_a, 'sim_classifier', epoch, save_dir)\n",
    "\n",
    "    # Anneal learning rate:\n",
    "    if epochs_without_improvement == opt.start_annealing:\n",
    "        old_learning_rate = learning_rate\n",
    "        learning_rate *= annealing_factor\n",
    "        update_learning_rate(classifier.optimizer_a, learning_rate)\n",
    "        print('Learning rate has been updated from %.4f to %.4f' % (old_learning_rate, learning_rate))\n",
    "\n",
    "    # Terminate training early, if no improvement has been observed for n epochs\n",
    "    if epochs_without_improvement >= opt.patience:\n",
    "        print('Stopping training early after %d epochs, following %d epochs without performance improvement.' %\n",
    "              (epoch, epochs_without_improvement))\n",
    "        final_epoch = epoch\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a.encoding_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(-torch.norm((self.encoding_a - self.encoding_b), 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.,\n",
       "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,  1.,\n",
       "         0.,  0.,  1.,  0.,  1.,  0.,  1.,  1.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1=a.encoding_a\n",
    "x2=a.encoding_b\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1894)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.3325,  0.4092,  0.3487,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.3281,  0.0000,  0.5214,  0.0000,  0.4586,  0.4730,\n",
       "         0.0000,  0.3504,  0.3467,  0.4573,  0.4311,  0.0000,  0.0000,\n",
       "         0.0000,  0.3379,  0.0000,  0.5104,  0.3919,  0.0000,  0.4083,\n",
       "         0.0000,  0.3175,  0.0000,  0.0000])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-7ad2e5c46943>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdist_sq\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "mdist = 1 - x\n",
    "dist = torch.clamp(mdist, min=0.0)\n",
    "loss = y * dist_sq + (1 - y) * torch.pow(dist, 2)\n",
    "loss = torch.sum(loss) / 2.0 / x1.size()[0]\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.out1=self.stackedRNN(self.embedded_words1, self.dropout_keep_prob, \"side1\", embedding_size, sequence_length, hidden_units)\n",
    "self.out2=self.stackedRNN(self.embedded_words2, self.dropout_keep_prob, \"side2\", embedding_size, sequence_length, hidden_units)\n",
    "self.distance = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(self.out1,self.out2)),1,keep_dims=True))\n",
    "self.distance = tf.div(self.distance, tf.add(tf.sqrt(tf.reduce_sum(tf.square(self.out1),1,keep_dims=True)),tf.sqrt(tf.reduce_sum(tf.square(self.out2),1,keep_dims=True))))\n",
    "self.distance = tf.reshape(self.distance, [-1], name=\"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "a=np.rand([12,3,4])\n",
    "b=np.rand([12,3,4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.subtract(a,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([15, 32, 300])\n",
      "torch.Size([1, 32, 50])\n",
      "torch.Size([15, 32, 300])\n",
      "torch.Size([1, 32, 50])\n",
      "1 torch.Size([1, 32, 50])\n",
      "1 torch.Size([32, 50])\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31337, 300])"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(a.encoder_a.parameters()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embedding_table.weight',\n",
       "              tensor([[ 1.0572e-04, -3.8022e-05,  1.0930e-02,  ..., -2.6759e-03,\n",
       "                       -5.1232e-03,  4.4218e-03],\n",
       "                      [-1.0982e-02,  1.2633e-02, -7.2347e-03,  ..., -7.0619e-03,\n",
       "                        1.2662e-02,  2.4631e-03],\n",
       "                      [-4.5400e-03, -8.2113e-03,  1.4869e-02,  ...,  4.3612e-03,\n",
       "                       -5.3701e-03, -1.8390e-03],\n",
       "                      ...,\n",
       "                      [-2.7418e-03,  1.2780e-03,  5.4348e-03,  ...,  1.2854e-03,\n",
       "                        2.6368e-03, -7.9226e-03],\n",
       "                      [-3.9866e-03,  9.2723e-03, -5.0916e-03,  ...,  8.7221e-03,\n",
       "                       -3.7669e-03,  5.8461e-03],\n",
       "                      [-1.5181e-03, -8.3555e-03, -7.7473e-03,  ...,  2.1644e-03,\n",
       "                       -1.2122e-02, -3.1460e-03]])),\n",
       "             ('lstm_rnn.weight_ih_l0',\n",
       "              tensor([[ 8.8490e-02,  2.7174e-02, -7.9495e-02,  ...,  4.9331e-02,\n",
       "                        2.1557e-02,  5.4980e-02],\n",
       "                      [ 1.9297e-02, -6.9169e-02, -6.9863e-02,  ...,  2.9669e-04,\n",
       "                        7.3090e-02,  3.4397e-02],\n",
       "                      [ 5.7891e-02,  2.3940e-02,  7.9111e-02,  ...,  7.8355e-02,\n",
       "                       -4.0976e-02,  4.0323e-03],\n",
       "                      ...,\n",
       "                      [-2.3771e-02,  1.6761e-02,  2.1230e-02,  ..., -5.2593e-03,\n",
       "                        7.1955e-02,  9.8631e-03],\n",
       "                      [-9.0141e-02,  5.1326e-02, -2.9428e-02,  ..., -4.5410e-02,\n",
       "                       -3.8724e-02, -2.2433e-02],\n",
       "                      [ 2.5776e-03,  8.8235e-03, -4.3434e-02,  ..., -1.5660e-02,\n",
       "                        2.1555e-02,  1.7488e-02]])),\n",
       "             ('lstm_rnn.weight_hh_l0',\n",
       "              tensor([[-6.3061e-02, -5.6062e-02,  9.6424e-02,  ..., -7.7395e-02,\n",
       "                        7.6997e-02, -1.6982e-02],\n",
       "                      [ 3.6698e-02, -4.1985e-02, -2.3617e-02,  ...,  3.0097e-02,\n",
       "                       -1.7806e-02,  1.7269e-02],\n",
       "                      [ 1.1839e-01,  5.7462e-02, -3.5079e-02,  ...,  2.0791e-02,\n",
       "                        3.5314e-02, -5.2038e-02],\n",
       "                      ...,\n",
       "                      [-7.2783e-02,  1.0242e-01, -1.4617e-01,  ..., -7.3058e-02,\n",
       "                        1.4561e-01,  3.3960e-02],\n",
       "                      [ 5.7767e-02,  6.3826e-02, -2.8144e-01,  ..., -5.6213e-02,\n",
       "                        6.8077e-02,  4.9978e-02],\n",
       "                      [ 3.3450e-02, -3.0920e-02, -1.6209e-01,  ...,  1.5396e-01,\n",
       "                       -8.3683e-02, -1.3802e-01]])),\n",
       "             ('lstm_rnn.bias_ih_l0',\n",
       "              tensor([-0.0963, -0.1313, -0.0080, -0.0211, -0.1215,  0.0211,  0.0492,\n",
       "                      -0.0605,  0.1283,  0.0207,  0.0518, -0.0803,  0.0878, -0.0394,\n",
       "                      -0.0361,  0.0562,  0.0607, -0.0785,  0.1095,  0.0466,  0.1194,\n",
       "                       0.0464,  0.1277,  0.0279,  0.1136, -0.0356,  0.1054,  0.0867,\n",
       "                       0.0545, -0.0638,  0.1162, -0.1300,  0.0562, -0.1385,  0.1193,\n",
       "                       0.0927, -0.0007,  0.0596,  0.1052, -0.0272,  0.1099, -0.1216,\n",
       "                       0.1328, -0.1378, -0.0015,  0.0245,  0.0178,  0.0236, -0.0140,\n",
       "                      -0.0627,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
       "                       2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
       "                       2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
       "                       2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
       "                       2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
       "                       2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
       "                       2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
       "                       2.5000,  2.5000,  0.0348, -0.1244, -0.0139,  0.0296,  0.0867,\n",
       "                       0.0812,  0.0606,  0.0869,  0.0051,  0.0328, -0.0504, -0.0820,\n",
       "                      -0.0207, -0.0609, -0.1081,  0.0888, -0.1310, -0.0700, -0.0469,\n",
       "                       0.0300,  0.1155,  0.0488,  0.0172, -0.0488, -0.0885,  0.0379,\n",
       "                       0.0263,  0.1106,  0.0805, -0.1026, -0.0598, -0.0398, -0.1180,\n",
       "                       0.0886, -0.0554, -0.0510, -0.0585,  0.1111, -0.0733, -0.0792,\n",
       "                      -0.1379, -0.0339, -0.0073, -0.0075, -0.1357, -0.0498, -0.1146,\n",
       "                      -0.0492, -0.0050, -0.0439,  0.0153, -0.1091,  0.0570, -0.0271,\n",
       "                       0.0240,  0.1180, -0.0301, -0.0421,  0.0249,  0.1053, -0.1263,\n",
       "                       0.1345,  0.0410,  0.0149, -0.0027,  0.1329,  0.1113,  0.1283,\n",
       "                      -0.0329, -0.0878, -0.0676, -0.0763,  0.1042,  0.0850,  0.1086,\n",
       "                       0.1032,  0.1047, -0.1051, -0.1376, -0.0522,  0.1350,  0.1022,\n",
       "                       0.0613,  0.0799, -0.0035, -0.1301, -0.1406,  0.0578,  0.0994,\n",
       "                      -0.1240, -0.0806, -0.0805, -0.0551, -0.0904,  0.0391, -0.0305,\n",
       "                       0.1226,  0.0674,  0.1036, -0.1087])),\n",
       "             ('lstm_rnn.bias_hh_l0',\n",
       "              tensor([ 0.0235,  0.0112, -0.1293, -0.1235, -0.0323, -0.0408, -0.0475,\n",
       "                      -0.0386,  0.0905, -0.0476,  0.0179, -0.1288, -0.1336, -0.0135,\n",
       "                      -0.0105,  0.1056,  0.0390,  0.0322, -0.1349,  0.0318,  0.0142,\n",
       "                       0.0273, -0.0378,  0.1103, -0.1028, -0.1315,  0.1242, -0.0339,\n",
       "                      -0.0312,  0.0625,  0.0496,  0.0731,  0.0205,  0.0732, -0.0133,\n",
       "                      -0.0966,  0.0929,  0.1060, -0.1210,  0.0843, -0.0244, -0.1149,\n",
       "                      -0.0985, -0.0132, -0.0226, -0.0590, -0.0283, -0.0045, -0.0437,\n",
       "                       0.0703,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
       "                       2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
       "                       2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
       "                       2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
       "                       2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
       "                       2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
       "                       2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
       "                       2.5000,  2.5000,  0.1005, -0.1145, -0.1123, -0.0980, -0.1006,\n",
       "                      -0.1180,  0.1219, -0.0233,  0.0109, -0.0788,  0.0898, -0.0369,\n",
       "                      -0.0630,  0.1243, -0.1198,  0.1218, -0.0107, -0.0375, -0.0666,\n",
       "                      -0.0883, -0.0263,  0.1286,  0.0159, -0.0838, -0.0802, -0.0823,\n",
       "                      -0.0129, -0.0282,  0.0179,  0.0733, -0.1315, -0.0509,  0.1018,\n",
       "                       0.1221,  0.0475,  0.0505,  0.1168,  0.0968, -0.1409,  0.0385,\n",
       "                      -0.0456, -0.0501,  0.1336, -0.1337, -0.0702,  0.0467,  0.0574,\n",
       "                       0.0300, -0.0543, -0.0205, -0.0133,  0.0687, -0.0446,  0.0535,\n",
       "                       0.1155, -0.1109, -0.1253, -0.1198,  0.0496, -0.1232,  0.0884,\n",
       "                       0.0344,  0.1110,  0.0137, -0.0527, -0.0241,  0.0219,  0.0359,\n",
       "                       0.0126,  0.1403, -0.1072,  0.0380, -0.1160,  0.1202,  0.0089,\n",
       "                       0.0748, -0.0336,  0.0666,  0.0465, -0.0435, -0.0310,  0.0293,\n",
       "                       0.0243, -0.0124,  0.0445, -0.1040, -0.0828,  0.0999, -0.0580,\n",
       "                      -0.0712,  0.0542, -0.0974, -0.0499, -0.0311,  0.0130, -0.1001,\n",
       "                      -0.0640,  0.0733, -0.0709, -0.1237]))])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = a.state_dict()\n",
    "for key in state_dict.keys():\n",
    "    if '.weight' in key:\n",
    "        state_dict[key] = xavier_normal(state_dict[key])\n",
    "    if '.bias' in key:\n",
    "        bias_length = state_dict[key].size()[0]\n",
    "        start, end = bias_length // 4, bias_length // 2\n",
    "        state_dict[key][start:end].fill_(2.5)\n",
    "state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n",
      "torch.Size([32, 300])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0080,  0.0900, -0.0215,  ..., -0.0209,  0.0209, -0.1176],\n",
       "         [-0.0080,  0.0900, -0.0215,  ..., -0.0209,  0.0209, -0.1176],\n",
       "         [-0.0080,  0.0900, -0.0215,  ..., -0.0209,  0.0209, -0.1176],\n",
       "         ...,\n",
       "         [-0.0080,  0.0900, -0.0215,  ..., -0.0209,  0.0209, -0.1176],\n",
       "         [-0.0080,  0.0900, -0.0215,  ..., -0.0209,  0.0209, -0.1176],\n",
       "         [-0.0080,  0.0900, -0.0215,  ..., -0.0209,  0.0209, -0.1176]]])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "class LSTMEncoder(nn.Module):\n",
    "    \"\"\" Implements the network type integrated within the Siamese RNN architecture. \"\"\"\n",
    "    def __init__(self, vocab_size, is_train=False):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.vocab_size=31337\n",
    "        self.hidden_dims=50\n",
    "        self.embedding_dims=300\n",
    "        self.num_layers=3\n",
    "        self.dropout_p = 1\n",
    "        self.is_train=is_train\n",
    "        \n",
    "        self.embedding_table = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_dims,\n",
    "                                            padding_idx=0, max_norm=None, scale_grad_by_freq=False, sparse=False)\n",
    "        self.lstm_rnn = nn.LSTM(input_size=self.embedding_dims, hidden_size=self.hidden_dims, num_layers=3)\n",
    "\n",
    "    def initialize_hidden_plus_cell(self, batch_size):\n",
    "        \"\"\" Re-initializes the hidden state, cell state, and the forget gate bias of the network. \"\"\"\n",
    "        zero_hidden = Variable(torch.randn(3, batch_size, self.hidden_dims))\n",
    "        zero_cell = Variable(torch.randn(3, batch_size, self.hidden_dims))\n",
    "        return zero_hidden, zero_cell\n",
    "\n",
    "    def forward(self, batch_size, input_data, hidden, cell):\n",
    "        \"\"\" Performs a forward pass through the network. \"\"\"\n",
    "        output = self.embedding_table(input_data)\n",
    "        print(output.shape)\n",
    "        \n",
    "        output=output.view(1, batch_size, -1)\n",
    "        output, (hidden, cell) = self.lstm_rnn(output, (hidden, cell))\n",
    "        return output, hidden, cell\n",
    "       \n",
    "vocab_size=31337\n",
    "hidden_dims=50\n",
    "embedding_dims=300\n",
    "num_layers=3\n",
    "dropout_p = 1.\n",
    "is_train=False\n",
    "batch_size=32\n",
    "        \n",
    "a=LSTMEncoder(vocab_size)\n",
    "hidden,cell=a.initialize_hidden_plus_cell(batch_size)\n",
    "\n",
    "for i in range (50):\n",
    "        output,hidden,cell=a.forward(batch_size,x1[:,i],hidden,cell)\n",
    "        #append\n",
    "output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 50])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4329,  0.7376,  0.3145]],\n",
       "\n",
       "        [[ 1.2968,  0.6176,  2.3458]],\n",
       "\n",
       "        [[ 0.9655,  0.5458,  0.4760]],\n",
       "\n",
       "        [[-0.9754, -0.7377,  0.0153]],\n",
       "\n",
       "        [[ 0.4082,  0.8577,  0.0263]]])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "#inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "torch.cat(inputs).view(len(inputs), 1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size,embedding_dims,hidden_dims,num_layers,dropout_p,is_train):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.hidden_dims=hidden_dims\n",
    "        self.embedding_dims=embedding_dims\n",
    "        self.num_layers=num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.is_train=is_train\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.embedding_table = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_dims,padding_idx=0, max_norm=None, \n",
    "                                       scale_grad_by_freq=False, sparse=False)\n",
    "        self.lstm_rnn = nn.LSTM(input_size=self.embedding_dims,hidden_size=self.hidden_dims, num_layers=1,batch_first=True)\n",
    "        self.dropout = nn.Dropout(self.dropout_p )\n",
    "        \n",
    "    def initHidden(self):\n",
    "        hidden_a = torch.randn(1, batch_size,hidden_dims)\n",
    "        hidden_b = torch.randn(1,batch_size,hidden_dims)\n",
    "        return (hidden_a,hidden_b)\n",
    "        \n",
    "    def forward(self,data,hidden):\n",
    "        output = embedding_table(data)\n",
    "        print(output.shape)\n",
    "        print(hidden[0].shape)\n",
    "       \n",
    "        output, hidden= self.lstm_rnn(output, hidden)\n",
    "        return output, hidden\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100, 300])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em=nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dims,padding_idx=0, max_norm=None, \n",
    "                                       scale_grad_by_freq=False, sparse=False)\n",
    "a=em(x1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-169-5ca4aecfdbc3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlstm\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membedding_dims\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mhidden\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "lstm= nn.LSTM(input_size=embedding_dims,hidden_size=hidden_dims, num_layers=1,batch_first=True)\n",
    "output, hidden= lstm(a, hidden)\n",
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 300, got 50",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-150-b986803ef843>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlstm_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    176\u001b[0m             \u001b[0mflat_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m         func = self._backend.RNN(\n\u001b[0;32m    180\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    128\u001b[0m             raise RuntimeError(\n\u001b[0;32m    129\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[1;32m--> 130\u001b[1;33m                     self.input_size, input.size(-1)))\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_input_packed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 300, got 50"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "embedding_table.weight.data.copy_(torch.tensor(initW))\n",
    "output = embedding_table(x1)\n",
    "output.shape\n",
    "\n",
    "for _ in range(num_layers):\n",
    "    output,hidden=lstm_rnn(output,hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-10-b12e7870e226>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-b12e7870e226>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, opt, is_train=False):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.opt = opt\n",
    "        self.name = 'sim_encoder'\n",
    "\n",
    "        # Layers\n",
    "        self.embedding_table = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.opt.embedding_dims,\n",
    "                                            padding_idx=0, max_norm=None, scale_grad_by_freq=False, sparse=False)\n",
    "        self.lstm_rnn = nn.LSTM(input_size=self.opt.embedding_dims, hidden_size=self.opt.hidden_dims, num_layers=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "\n",
    "# Inference with SVR\n",
    "import pickle\n",
    "\n",
    "from utils.parameter_initialization import xavier_normal\n",
    "\n",
    "\n",
    "class LSTMEncoder(nn.Module):\n",
    "    \"\"\" Implements the network type integrated within the Siamese RNN architecture. \"\"\"\n",
    "    def __init__(self, vocab_size, opt, is_train=False):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.opt = opt\n",
    "        self.name = 'sim_encoder'\n",
    "\n",
    "        # Layers\n",
    "        self.embedding_table = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.opt.embedding_dims,\n",
    "                                            padding_idx=0, max_norm=None, scale_grad_by_freq=False, sparse=False)\n",
    "        self.lstm_rnn = nn.LSTM(input_size=self.opt.embedding_dims, hidden_size=self.opt.hidden_dims, num_layers=1)\n",
    "\n",
    "    def initialize_hidden_plus_cell(self, batch_size):\n",
    "        \"\"\" Re-initializes the hidden state, cell state, and the forget gate bias of the network. \"\"\"\n",
    "        zero_hidden = Variable(torch.randn(1, batch_size, self.opt.hidden_dims))\n",
    "        zero_cell = Variable(torch.randn(1, batch_size, self.opt.hidden_dims))\n",
    "        return zero_hidden, zero_cell\n",
    "\n",
    "    def forward(self, batch_size, input_data, hidden, cell):\n",
    "        \"\"\" Performs a forward pass through the network. \"\"\"\n",
    "        output = self.embedding_table(input_data).view(1, batch_size, -1)\n",
    "        for _ in range(self.opt.num_layers):\n",
    "            output, (hidden, cell) = self.lstm_rnn(output, (hidden, cell))\n",
    "        return output, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils.parameter_initialization import _calculate_fan_in_and_fan_out,xavier_uniform,xavier_normal\n",
    "vocab_size=500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31337"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.randn([vocab_size, 300])\n",
    "fan_in,fan_out=_calculate_fan_in_and_fan_out(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init_embeddings = xavier_normal(torch.randn([vocab_size, 300]))\n",
    "std = 1* np.sqrt(2.0 / (fan_in + fan_out))\n",
    "b=a.normal_(0, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.9485e-03,  2.4569e-02, -1.2476e-03,  ..., -7.2306e-02,\n",
       "          8.3485e-03, -3.2094e-02],\n",
       "        [-2.4150e-02,  4.6721e-02, -1.8776e-02,  ..., -1.8566e-02,\n",
       "         -2.8833e-02,  3.3286e-02],\n",
       "        [-9.9432e-02, -5.7579e-03,  3.3980e-02,  ..., -9.8987e-02,\n",
       "         -2.7271e-02,  5.1338e-03],\n",
       "        ...,\n",
       "        [ 5.0814e-02,  5.2567e-02, -3.9122e-02,  ...,  1.0214e-01,\n",
       "          3.9445e-02,  2.6966e-02],\n",
       "        [ 2.1296e-02,  3.2408e-02, -2.9215e-02,  ...,  1.3672e-02,\n",
       "         -5.2742e-02, -3.0810e-02],\n",
       "        [ 5.3334e-02,  5.1146e-02, -5.9623e-02,  ...,  5.9365e-02,\n",
       "          7.6943e-02,  1.6302e-02]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
