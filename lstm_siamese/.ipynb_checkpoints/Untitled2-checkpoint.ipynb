{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "from input_helpers import InputHelper\n",
    "from siamese_network import SiameseLSTM\n",
    "from siamese_network_semantic import SiameseLSTMw2v\n",
    "from tensorflow.contrib import learn\n",
    "import gzip\n",
    "from random import random\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "class SiameseLSTMw2v(object):\n",
    "    \"\"\"\n",
    "    A LSTM based deep Siamese network for text similarity.\n",
    "    Uses an word embedding layer (looks up in pre-trained w2v), followed by a biLSTM and Energy Loss layer.\n",
    "    \"\"\"\n",
    "    #train model in main \n",
    "    #sess.run(siameseModel.W.assign(initW))\n",
    "    \n",
    "# word2vec_format=\"bin\"\n",
    "# embedding_dim=300\n",
    "# dropout_keep_prob=1.0\n",
    "# l2_reg_lambda=0.0\n",
    "# hidden_units=50\n",
    "\n",
    "    \n",
    "#      siameseModel = SiameseLSTMw2v(\n",
    "#                 sequence_length=max_document_length,\n",
    "#                 vocab_size=len(vocab_processor.vocabulary_),\n",
    "#                 embedding_size=FLAGS.embedding_dim,\n",
    "#                 hidden_units=FLAGS.hidden_units,\n",
    "#                 l2_reg_lambda=FLAGS.l2_reg_lambda,\n",
    "#                 batch_size=FLAGS.batch_size,\n",
    "#                 trainableEmbeddings=trainableEmbeddings\n",
    "#             )\n",
    "#         # Define Training procedure\n",
    "#         global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "#         optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "#         print(\"initialized siameseModel object\")\n",
    "    \n",
    "  #  grads_and_vars=optimizer.compute_gradients(siameseModel.loss)\n",
    "  #  tr_op_set = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "    \n",
    "    def stackedRNN(self, x, dropout, scope, embedding_size, sequence_length, hidden_units):\n",
    "        n_hidden=hidden_units\n",
    "        n_layers=3\n",
    "        # Prepare data shape to match `static_rnn` function requirements\n",
    "        x = tf.unstack(x,axis=1)\n",
    "        \n",
    "        # Define lstm cells with tensorflow\n",
    "        # Forward direction cell\n",
    "\n",
    "        with tf.name_scope(\"fw\"+scope),tf.variable_scope(\"fw\"+scope):\n",
    "            stacked_rnn_fw = []\n",
    "            for _ in range(n_layers):\n",
    "                fw_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "                lstm_fw_cell = tf.contrib.rnn.DropoutWrapper(fw_cell,output_keep_prob=dropout)\n",
    "                stacked_rnn_fw.append(lstm_fw_cell)\n",
    "            lstm_fw_cell_m = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn_fw, state_is_tuple=True)\n",
    "            #print (\"lstm shape is \"+ lstm_fw_cell_m.shape)\n",
    "            outputs, _ = tf.nn.static_rnn(lstm_fw_cell_m, x, dtype=tf.float32)\n",
    "            #print (\"output length is \"+ len(outputs))\n",
    "            print (\"output shape is \"+ str(outputs[-1].shape))\n",
    "        return outputs[-1]\n",
    "\n",
    "    def contrastive_loss(self, y,d,batch_size):\n",
    "        tmp= y *tf.square(d) ## when they are almost the same y=1, the square means they are increasing \n",
    "        #tmp= tf.mul(y,tf.square(d))\n",
    "        tmp2 = (1-y) *tf.square(tf.maximum((1 - d),0)) \n",
    "        ## hinge loss  when they are not same, the function is decreasing monotonically\n",
    "        return tf.reduce_sum(tmp +tmp2)/batch_size/2\n",
    "    \n",
    "    def __init__(\n",
    "        self, sequence_length, vocab_size, embedding_size, hidden_units, l2_reg_lambda, batch_size, trainableEmbeddings):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x1 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x1\")\n",
    "        self.input_x2 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x2\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0, name=\"l2_loss\")\n",
    "          \n",
    "        # Embedding layer\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.constant(0.0, shape=[vocab_size, embedding_size]),\n",
    "                trainable=trainableEmbeddings,name=\"W\")\n",
    "            self.embedded_words1 = tf.nn.embedding_lookup(self.W, self.input_x1) # dim:(sequence,word_count,embed_dim)\n",
    "            self.embedded_words2 = tf.nn.embedding_lookup(self.W, self.input_x2) # dim:(sequence,word_count,embed_dim)\n",
    "        print (\"embeded shape is \"+str(self.embedded_words1.shape ))\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        with tf.name_scope(\"output\"):\n",
    "            self.out1=self.stackedRNN(self.embedded_words1, self.dropout_keep_prob, \"side1\", embedding_size, sequence_length, hidden_units)\n",
    "            self.out2=self.stackedRNN(self.embedded_words2, self.dropout_keep_prob, \"side2\", embedding_size, sequence_length, hidden_units)\n",
    "            self.distance = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(self.out1,self.out2)),1,keep_dims=True))\n",
    "            self.distance = tf.div(self.distance, tf.add(tf.sqrt(tf.reduce_sum(tf.square(self.out1),1,keep_dims=True)),tf.sqrt(tf.reduce_sum(tf.square(self.out2),1,keep_dims=True))))\n",
    "            self.distance = tf.reshape(self.distance, [-1], name=\"distance\")\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.loss = self.contrastive_loss(self.input_y,self.distance, batch_size)\n",
    "        #### Accuracy computation is outside of this class.\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.temp_sim = tf.subtract(tf.ones_like(self.distance),tf.rint(self.distance), name=\"temp_sim\") #auto threshold 0.5\n",
    "            correct_predictions = tf.equal(self.temp_sim, self.input_y)\n",
    "            self.accuracy=tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from train_snli.txt\n",
      "Building vocabulary\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\deep-siamese-text-similarity-master\\preprocess.py:34: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From C:\\Users\\pc\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "Length of loaded vocabulary =31337\n",
      "dumping validation 0\n",
      "Train/Dev split for train_snli.txt: 330635/36738\n",
      "Loading W2V data...\n",
      "loaded word2vec len  111051\n",
      "initializing initW with pre-trained word2vec embeddings\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "is_char_based=False\n",
    "word2vec_model=\"D:\\simple_vec\\wiki.simple.bin\"\n",
    "word2vec_format=\"bin\"\n",
    "embedding_dim=300\n",
    "dropout_keep_prob=1.0\n",
    "l2_reg_lambda=0.0\n",
    "hidden_units=50\n",
    "\n",
    "# Training parameters\n",
    "batch_size=64\n",
    "num_epochs=300\n",
    "evaluate_every=1000\n",
    "checkpoint_every=1000\n",
    "\n",
    "# Misc Parameters\n",
    "allow_soft_placement=True\n",
    "log_device_placement=False\n",
    "trainableEmbeddings=False\n",
    "\n",
    "training_files=\"train_snli.txt\"\n",
    "\n",
    "max_document_length=100\n",
    "inpH = InputHelper()\n",
    "train_set, dev_set, vocab_processor,sum_no_of_batches = inpH.getDataSets(training_files, 10,max_document_length,batch_size, is_char_based)\n",
    "trainableEmbeddings=False\n",
    "if is_char_based==True:\n",
    "    word2vec_model = False\n",
    "\n",
    "inpH.loadW2V(word2vec_model, word2vec_format)\n",
    "\n",
    "\n",
    "# initial matrix with random uniform\n",
    "initW = np.random.uniform(-0.25,0.25,(len(vocab_processor.vocabulary_), embedding_dim))\n",
    "#initW = np.zeros(shape=(len(vocab_processor.vocabulary_), FLAGS.embedding_dim))\n",
    "# load any vectors from the word2vec\n",
    "print(\"initializing initW with pre-trained word2vec embeddings\")\n",
    "\n",
    "\n",
    "for w in vocab_processor.vocabulary_._mapping:\n",
    "    arr=[]\n",
    "    s = re.sub('[^0-9a-zA-Z]+', '', w)\n",
    "    if w in inpH.pre_emb:\n",
    "        arr=inpH.pre_emb[w]\n",
    "    elif w.lower() in inpH.pre_emb:\n",
    "        arr=inpH.pre_emb[w.lower()]\n",
    "    elif s in inpH.pre_emb:\n",
    "        arr=inpH.pre_emb[s]\n",
    "    elif s.isdigit():\n",
    "        arr=inpH.pre_emb[\"zero\"]\n",
    "    if len(arr)>0:\n",
    "        idx = vocab_processor.vocabulary_.get(w)\n",
    "        initW[idx]=np.asarray(arr).astype(np.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started session\n",
      "embeded shape is (?, 100, 300)\n",
      "output shape is (?, 50)\n",
      "output shape is (?, 50)\n",
      "WARNING:tensorflow:From <ipython-input-1-380968c7758c>:104: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "initialized siameseModel object\n",
      "defined training_ops\n",
      "Writing to C:\\Users\\pc\\deep-siamese-text-similarity-master\\runs\\1532596722\n",
      "\n",
      "init all variables\n",
      "initializing initW with pre-trained word2vec embeddings\n",
      "Done assigning intiW. len=31337\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "session_conf = tf.ConfigProto(\n",
    "  allow_soft_placement=allow_soft_placement,\n",
    "  log_device_placement=log_device_placement)\n",
    "sess = tf.Session(config=session_conf)\n",
    "print(\"started session\")\n",
    "trainableEmbeddings=False\n",
    "with sess.as_default():\n",
    "    if is_char_based:\n",
    "        siameseModel = SiameseLSTM(\n",
    "            sequence_length=max_document_length,\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=embedding_dim,\n",
    "            hidden_units=hidden_units,\n",
    "            l2_reg_lambda=l2_reg_lambda,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "    else:\n",
    "        siameseModel = SiameseLSTMw2v(\n",
    "            sequence_length=max_document_length,\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=embedding_dim,\n",
    "            hidden_units=hidden_units,\n",
    "            l2_reg_lambda=l2_reg_lambda,\n",
    "            batch_size=batch_size,\n",
    "            trainableEmbeddings=trainableEmbeddings\n",
    "        )\n",
    "    # Define Training procedure\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    print(\"initialized siameseModel object\")\n",
    "\n",
    "grads_and_vars=optimizer.compute_gradients(siameseModel.loss)\n",
    "tr_op_set = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "print(\"defined training_ops\")\n",
    "\n",
    "\n",
    "# Output directory for models and summaries\n",
    "timestamp = str(int(time.time()))\n",
    "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "#Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=100)\n",
    "\n",
    "# Initialize all variables\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"init all variables\")\n",
    "graph_def = tf.get_default_graph().as_graph_def()\n",
    "graphpb_txt = str(graph_def)\n",
    "\n",
    "\n",
    "# initial matrix with random uniform\n",
    "initW = np.random.uniform(-0.25,0.25,(len(vocab_processor.vocabulary_), embedding_dim))\n",
    "#initW = np.zeros(shape=(len(vocab_processor.vocabulary_), FLAGS.embedding_dim))\n",
    "# load any vectors from the word2vec\n",
    "print(\"initializing initW with pre-trained word2vec embeddings\")\n",
    "for w in vocab_processor.vocabulary_._mapping:\n",
    "    arr=[]\n",
    "    s = re.sub('[^0-9a-zA-Z]+', '', w)\n",
    "    if w in inpH.pre_emb:\n",
    "        arr=inpH.pre_emb[w]\n",
    "    elif w.lower() in inpH.pre_emb:\n",
    "        arr=inpH.pre_emb[w.lower()]\n",
    "    elif s in inpH.pre_emb:\n",
    "        arr=inpH.pre_emb[s]\n",
    "    if len(arr)>0:\n",
    "        idx = vocab_processor.vocabulary_.get(w)\n",
    "        initW[idx]=np.asarray(arr).astype(np.float32)\n",
    "print(\"Done assigning intiW. len=\"+str(len(initW)))\n",
    "inpH.deletePreEmb()\n",
    "gc.collect()\n",
    "sess.run(siameseModel.W.assign(initW))\n",
    "\n",
    "def train_step(x1_batch, x2_batch, y_batch):\n",
    "    \"\"\"\n",
    "    A single training step\n",
    "    \"\"\"\n",
    "    if random()>0.5:\n",
    "        feed_dict = {\n",
    "            siameseModel.input_x1: x1_batch,\n",
    "            siameseModel.input_x2: x2_batch,\n",
    "            siameseModel.input_y: y_batch,\n",
    "            siameseModel.dropout_keep_prob: dropout_keep_prob,\n",
    "        }\n",
    "    else:\n",
    "        feed_dict = {\n",
    "            siameseModel.input_x1: x2_batch,\n",
    "            siameseModel.input_x2: x1_batch,\n",
    "            siameseModel.input_y: y_batch,\n",
    "            siameseModel.dropout_keep_prob: dropout_keep_prob,\n",
    "        }\n",
    "    _, step, loss, accuracy, dist, sim = sess.run([tr_op_set, global_step, siameseModel.loss, siameseModel.accuracy, siameseModel.distance, siameseModel.temp_sim],  feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"TRAIN {}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "    #print(y_batch, dist, sim)\n",
    "\n",
    "def dev_step(x1_batch, x2_batch, y_batch):\n",
    "    \"\"\"\n",
    "    A single training step\n",
    "    \"\"\" \n",
    "    if random()>0.5:\n",
    "        feed_dict = {\n",
    "            siameseModel.input_x1: x1_batch,\n",
    "            siameseModel.input_x2: x2_batch,\n",
    "            siameseModel.input_y: y_batch,\n",
    "            siameseModel.dropout_keep_prob: 0.9,\n",
    "        }\n",
    "    else:\n",
    "        feed_dict = {\n",
    "            siameseModel.input_x1: x2_batch,\n",
    "            siameseModel.input_x2: x1_batch,\n",
    "            siameseModel.input_y: y_batch,\n",
    "            siameseModel.dropout_keep_prob: 1.0,\n",
    "        }\n",
    "    step, loss, accuracy, sim= sess.run([global_step, siameseModel.loss, siameseModel.accuracy, siameseModel.temp_sim],  feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"DEV {}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "    #dev_summary_writer.add_summary(summaries, step)\n",
    "    #print (y_batch, sim)\n",
    "    return accuracy\n",
    "\n",
    "# Generate batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([   1,    2,    3, 1533,    9,    1, 1535,  318,   41,    2, 2963,\n",
      "        145,  528,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0], dtype=int64)\n",
      "  array([   28,     2,     3, 22763,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0], dtype=int64)\n",
      "  1]\n",
      " [array([   1,   51,  158,    1,  727,  187,  231,  580,  249,   27,   28,\n",
      "        600,   21,    3, 3869,   27,    1, 3912,  476,   28,  636,  152,\n",
      "        231,  367,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0], dtype=int64)\n",
      "  array([  1,  51,   3, 103, 158,   1, 727,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int64)\n",
      "  0]\n",
      " [array([  24,    3, 5757,    1, 1627,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0], dtype=int64)\n",
      "  array([   1,   24,   27,    1,   31, 1045,  295,    9,   28,  114,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0], dtype=int64)\n",
      "  0]\n",
      " ...\n",
      " [array([   1,   51,   27,    1,   55,   56, 1767,   86,  912,   28, 2146,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0], dtype=int64)\n",
      "  array([   1,  115, 3996,   28, 2146,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0], dtype=int64)\n",
      "  0]\n",
      " [array([  28,   51,    3, 5010, 1243,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0], dtype=int64)\n",
      "  array([   7,  635,  657, 5409, 1243,  110,    1,  781,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0], dtype=int64)\n",
      "  1]\n",
      " [array([   1,   51,   27,    1, 1126,  206,   28, 1160,   30,    1, 1898,\n",
      "        695,   86,   91, 1117,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0], dtype=int64)\n",
      "  array([   1,   51,   27,    1, 1126,  206,   28, 1160,   30,    1, 1898,\n",
      "        681,  187,    1,  330, 1683,  684,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0], dtype=int64)\n",
      "  0]]\n",
      "(330635, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN 2018-07-26T17:19:48.846366: step 1, loss 0.139178, acc 0.546875\n",
      "TRAIN 2018-07-26T17:19:49.642676: step 2, loss 0.12708, acc 0.484375\n",
      "TRAIN 2018-07-26T17:19:49.861427: step 3, loss 0.122953, acc 0.578125\n",
      "TRAIN 2018-07-26T17:19:50.085293: step 4, loss 0.13272, acc 0.515625\n",
      "TRAIN 2018-07-26T17:19:50.286599: step 5, loss 0.12469, acc 0.546875\n",
      "TRAIN 2018-07-26T17:19:50.497568: step 6, loss 0.122795, acc 0.578125\n",
      "TRAIN 2018-07-26T17:19:50.698529: step 7, loss 0.124881, acc 0.515625\n",
      "TRAIN 2018-07-26T17:19:50.890072: step 8, loss 0.124837, acc 0.53125\n",
      "TRAIN 2018-07-26T17:19:51.132081: step 9, loss 0.124921, acc 0.515625\n",
      "TRAIN 2018-07-26T17:19:51.359791: step 10, loss 0.12411, acc 0.578125\n",
      "TRAIN 2018-07-26T17:19:51.587254: step 11, loss 0.125158, acc 0.515625\n",
      "TRAIN 2018-07-26T17:19:51.773696: step 12, loss 0.123935, acc 0.546875\n",
      "TRAIN 2018-07-26T17:19:51.972753: step 13, loss 0.126123, acc 0.515625\n",
      "TRAIN 2018-07-26T17:19:52.176720: step 14, loss 0.119639, acc 0.625\n",
      "TRAIN 2018-07-26T17:19:52.369366: step 15, loss 0.123053, acc 0.5625\n",
      "TRAIN 2018-07-26T17:19:52.570715: step 16, loss 0.129261, acc 0.484375\n",
      "TRAIN 2018-07-26T17:19:52.761750: step 17, loss 0.12972, acc 0.453125\n",
      "TRAIN 2018-07-26T17:19:52.970884: step 18, loss 0.129179, acc 0.375\n",
      "TRAIN 2018-07-26T17:19:53.208945: step 19, loss 0.125397, acc 0.40625\n",
      "TRAIN 2018-07-26T17:19:53.390955: step 20, loss 0.125236, acc 0.46875\n",
      "TRAIN 2018-07-26T17:19:53.590955: step 21, loss 0.125483, acc 0.46875\n",
      "TRAIN 2018-07-26T17:19:53.787079: step 22, loss 0.125315, acc 0.5\n",
      "TRAIN 2018-07-26T17:19:53.986060: step 23, loss 0.122659, acc 0.59375\n",
      "TRAIN 2018-07-26T17:19:54.217118: step 24, loss 0.130474, acc 0.4375\n",
      "TRAIN 2018-07-26T17:19:54.436163: step 25, loss 0.123908, acc 0.546875\n",
      "TRAIN 2018-07-26T17:19:54.699230: step 26, loss 0.125897, acc 0.484375\n",
      "TRAIN 2018-07-26T17:19:54.963306: step 27, loss 0.124922, acc 0.53125\n",
      "TRAIN 2018-07-26T17:19:55.200360: step 28, loss 0.125637, acc 0.453125\n",
      "TRAIN 2018-07-26T17:19:55.442422: step 29, loss 0.125538, acc 0.421875\n",
      "TRAIN 2018-07-26T17:19:55.619921: step 30, loss 0.123763, acc 0.5625\n",
      "TRAIN 2018-07-26T17:19:55.828333: step 31, loss 0.133252, acc 0.40625\n",
      "TRAIN 2018-07-26T17:19:56.023638: step 32, loss 0.12723, acc 0.484375\n",
      "TRAIN 2018-07-26T17:19:56.254085: step 33, loss 0.126759, acc 0.4375\n",
      "TRAIN 2018-07-26T17:19:56.471164: step 34, loss 0.125998, acc 0.46875\n",
      "TRAIN 2018-07-26T17:19:56.672130: step 35, loss 0.130168, acc 0.421875\n",
      "TRAIN 2018-07-26T17:19:56.855479: step 36, loss 0.124621, acc 0.53125\n",
      "TRAIN 2018-07-26T17:19:57.073010: step 37, loss 0.126395, acc 0.484375\n",
      "TRAIN 2018-07-26T17:19:57.260510: step 38, loss 0.124902, acc 0.515625\n",
      "TRAIN 2018-07-26T17:19:57.462886: step 39, loss 0.12511, acc 0.484375\n",
      "TRAIN 2018-07-26T17:19:57.639327: step 40, loss 0.124671, acc 0.53125\n",
      "TRAIN 2018-07-26T17:19:57.840812: step 41, loss 0.12613, acc 0.484375\n",
      "TRAIN 2018-07-26T17:19:58.039212: step 42, loss 0.129919, acc 0.40625\n",
      "TRAIN 2018-07-26T17:19:58.234354: step 43, loss 0.126385, acc 0.453125\n",
      "TRAIN 2018-07-26T17:19:58.468415: step 44, loss 0.124838, acc 0.53125\n",
      "TRAIN 2018-07-26T17:19:58.681394: step 45, loss 0.121812, acc 0.609375\n",
      "TRAIN 2018-07-26T17:19:58.903008: step 46, loss 0.127339, acc 0.5\n",
      "TRAIN 2018-07-26T17:19:59.142167: step 47, loss 0.120638, acc 0.59375\n",
      "TRAIN 2018-07-26T17:19:59.344004: step 48, loss 0.126777, acc 0.53125\n",
      "TRAIN 2018-07-26T17:19:59.573923: step 49, loss 0.134827, acc 0.453125\n",
      "TRAIN 2018-07-26T17:19:59.796517: step 50, loss 0.131789, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:00.007524: step 51, loss 0.129474, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:00.212470: step 52, loss 0.127062, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:00.424781: step 53, loss 0.125026, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:00.689849: step 54, loss 0.123788, acc 0.5625\n",
      "TRAIN 2018-07-26T17:20:00.870544: step 55, loss 0.125722, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:01.075320: step 56, loss 0.128223, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:01.260223: step 57, loss 0.131937, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:01.456749: step 58, loss 0.134173, acc 0.4375\n",
      "TRAIN 2018-07-26T17:20:01.720268: step 59, loss 0.124193, acc 0.546875\n",
      "TRAIN 2018-07-26T17:20:01.983725: step 60, loss 0.118618, acc 0.640625\n",
      "TRAIN 2018-07-26T17:20:02.204980: step 61, loss 0.127131, acc 0.484375\n",
      "TRAIN 2018-07-26T17:20:02.404836: step 62, loss 0.127989, acc 0.453125\n",
      "TRAIN 2018-07-26T17:20:02.589643: step 63, loss 0.124513, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:02.772390: step 64, loss 0.124911, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:02.987131: step 65, loss 0.125652, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:03.208647: step 66, loss 0.124641, acc 0.546875\n",
      "TRAIN 2018-07-26T17:20:03.418853: step 67, loss 0.124948, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:03.676920: step 68, loss 0.124931, acc 0.578125\n",
      "TRAIN 2018-07-26T17:20:03.931122: step 69, loss 0.125149, acc 0.453125\n",
      "TRAIN 2018-07-26T17:20:04.117725: step 70, loss 0.124838, acc 0.5625\n",
      "TRAIN 2018-07-26T17:20:04.316214: step 71, loss 0.124628, acc 0.5625\n",
      "TRAIN 2018-07-26T17:20:04.529339: step 72, loss 0.124668, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:04.721563: step 73, loss 0.126949, acc 0.421875\n",
      "TRAIN 2018-07-26T17:20:04.961213: step 74, loss 0.123855, acc 0.5625\n",
      "TRAIN 2018-07-26T17:20:05.190618: step 75, loss 0.12372, acc 0.5625\n",
      "TRAIN 2018-07-26T17:20:05.375337: step 76, loss 0.126004, acc 0.484375\n",
      "TRAIN 2018-07-26T17:20:05.575431: step 77, loss 0.127667, acc 0.4375\n",
      "TRAIN 2018-07-26T17:20:05.782698: step 78, loss 0.124948, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:05.971766: step 79, loss 0.12325, acc 0.59375\n",
      "TRAIN 2018-07-26T17:20:06.156240: step 80, loss 0.124565, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:06.361641: step 81, loss 0.124558, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:06.549142: step 82, loss 0.127075, acc 0.421875\n",
      "TRAIN 2018-07-26T17:20:06.740944: step 83, loss 0.125898, acc 0.453125\n",
      "TRAIN 2018-07-26T17:20:06.957594: step 84, loss 0.125099, acc 0.484375\n",
      "TRAIN 2018-07-26T17:20:07.176888: step 85, loss 0.125194, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:07.361805: step 86, loss 0.123676, acc 0.609375\n",
      "TRAIN 2018-07-26T17:20:07.556460: step 87, loss 0.126117, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:07.772303: step 88, loss 0.125024, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:07.998176: step 89, loss 0.12513, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:08.212289: step 90, loss 0.121975, acc 0.59375\n",
      "TRAIN 2018-07-26T17:20:08.407794: step 91, loss 0.126113, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:08.606347: step 92, loss 0.124681, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:08.805670: step 93, loss 0.126288, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:08.989653: step 94, loss 0.1285, acc 0.453125\n",
      "TRAIN 2018-07-26T17:20:09.197750: step 95, loss 0.127229, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:09.393037: step 96, loss 0.124, acc 0.546875\n",
      "TRAIN 2018-07-26T17:20:09.589383: step 97, loss 0.123756, acc 0.5625\n",
      "TRAIN 2018-07-26T17:20:09.800851: step 98, loss 0.124274, acc 0.546875\n",
      "TRAIN 2018-07-26T17:20:10.000929: step 99, loss 0.126148, acc 0.4375\n",
      "TRAIN 2018-07-26T17:20:10.232997: step 100, loss 0.124898, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:10.482251: step 101, loss 0.125425, acc 0.375\n",
      "TRAIN 2018-07-26T17:20:10.711873: step 102, loss 0.123347, acc 0.703125\n",
      "TRAIN 2018-07-26T17:20:10.907562: step 103, loss 0.123209, acc 0.578125\n",
      "TRAIN 2018-07-26T17:20:11.113184: step 104, loss 0.124652, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:11.308503: step 105, loss 0.126107, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:11.569573: step 106, loss 0.132922, acc 0.4375\n",
      "TRAIN 2018-07-26T17:20:11.836641: step 107, loss 0.124529, acc 0.546875\n",
      "TRAIN 2018-07-26T17:20:12.058738: step 108, loss 0.137797, acc 0.390625\n",
      "TRAIN 2018-07-26T17:20:12.335810: step 109, loss 0.123161, acc 0.5625\n",
      "TRAIN 2018-07-26T17:20:12.563042: step 110, loss 0.126417, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:12.795798: step 111, loss 0.126998, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:12.988482: step 112, loss 0.12645, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:13.173110: step 113, loss 0.125949, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:13.393536: step 114, loss 0.12605, acc 0.484375\n",
      "TRAIN 2018-07-26T17:20:13.594778: step 115, loss 0.124221, acc 0.546875\n",
      "TRAIN 2018-07-26T17:20:13.786785: step 116, loss 0.12409, acc 0.578125\n",
      "TRAIN 2018-07-26T17:20:13.972653: step 117, loss 0.124911, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:14.201768: step 118, loss 0.125006, acc 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN 2018-07-26T17:20:14.448832: step 119, loss 0.125, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:14.681893: step 120, loss 0.124898, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:14.946961: step 121, loss 0.125028, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:15.158733: step 122, loss 0.126392, acc 0.375\n",
      "TRAIN 2018-07-26T17:20:15.401795: step 123, loss 0.124247, acc 0.609375\n",
      "TRAIN 2018-07-26T17:20:15.656862: step 124, loss 0.125311, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:15.929935: step 125, loss 0.124906, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:16.162961: step 126, loss 0.124907, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:16.412892: step 127, loss 0.125543, acc 0.4375\n",
      "TRAIN 2018-07-26T17:20:16.603894: step 128, loss 0.125293, acc 0.4375\n",
      "TRAIN 2018-07-26T17:20:16.805612: step 129, loss 0.125044, acc 0.484375\n",
      "TRAIN 2018-07-26T17:20:16.988150: step 130, loss 0.125033, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:17.189808: step 131, loss 0.125082, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:17.392745: step 132, loss 0.124372, acc 0.546875\n",
      "TRAIN 2018-07-26T17:20:17.589618: step 133, loss 0.123893, acc 0.5625\n",
      "TRAIN 2018-07-26T17:20:17.829355: step 134, loss 0.120894, acc 0.65625\n",
      "TRAIN 2018-07-26T17:20:18.019129: step 135, loss 0.128095, acc 0.453125\n",
      "TRAIN 2018-07-26T17:20:18.238784: step 136, loss 0.127979, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:18.525858: step 137, loss 0.128815, acc 0.453125\n",
      "TRAIN 2018-07-26T17:20:18.717997: step 138, loss 0.128168, acc 0.453125\n",
      "TRAIN 2018-07-26T17:20:18.921326: step 139, loss 0.12716, acc 0.453125\n",
      "TRAIN 2018-07-26T17:20:19.119742: step 140, loss 0.123669, acc 0.578125\n",
      "TRAIN 2018-07-26T17:20:19.324031: step 141, loss 0.125056, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:19.504436: step 142, loss 0.125163, acc 0.4375\n",
      "TRAIN 2018-07-26T17:20:19.715875: step 143, loss 0.124696, acc 0.546875\n",
      "TRAIN 2018-07-26T17:20:19.905013: step 144, loss 0.125402, acc 0.484375\n",
      "TRAIN 2018-07-26T17:20:20.104396: step 145, loss 0.126385, acc 0.453125\n",
      "TRAIN 2018-07-26T17:20:20.321006: step 146, loss 0.125352, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:20.507494: step 147, loss 0.122779, acc 0.59375\n",
      "TRAIN 2018-07-26T17:20:20.713080: step 148, loss 0.126471, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:20.924108: step 149, loss 0.127501, acc 0.4375\n",
      "TRAIN 2018-07-26T17:20:21.122213: step 150, loss 0.12407, acc 0.546875\n",
      "TRAIN 2018-07-26T17:20:21.366901: step 151, loss 0.124932, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:21.597080: step 152, loss 0.123814, acc 0.5625\n",
      "TRAIN 2018-07-26T17:20:21.803765: step 153, loss 0.124551, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:22.009644: step 154, loss 0.125581, acc 0.484375\n",
      "TRAIN 2018-07-26T17:20:22.203809: step 155, loss 0.123958, acc 0.5625\n",
      "TRAIN 2018-07-26T17:20:22.403511: step 156, loss 0.126426, acc 0.4375\n",
      "TRAIN 2018-07-26T17:20:22.638512: step 157, loss 0.126897, acc 0.390625\n",
      "TRAIN 2018-07-26T17:20:22.835837: step 158, loss 0.124311, acc 0.59375\n",
      "TRAIN 2018-07-26T17:20:23.070054: step 159, loss 0.124836, acc 0.546875\n",
      "TRAIN 2018-07-26T17:20:23.253727: step 160, loss 0.124944, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:23.480735: step 161, loss 0.125047, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:23.680208: step 162, loss 0.125004, acc 0.484375\n",
      "TRAIN 2018-07-26T17:20:23.908460: step 163, loss 0.124947, acc 0.546875\n",
      "TRAIN 2018-07-26T17:20:24.106695: step 164, loss 0.125008, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:24.306484: step 165, loss 0.125509, acc 0.421875\n",
      "TRAIN 2018-07-26T17:20:24.505727: step 166, loss 0.124646, acc 0.578125\n",
      "TRAIN 2018-07-26T17:20:24.689130: step 167, loss 0.125223, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:24.910894: step 168, loss 0.125321, acc 0.453125\n",
      "TRAIN 2018-07-26T17:20:25.088369: step 169, loss 0.125385, acc 0.40625\n",
      "TRAIN 2018-07-26T17:20:25.290708: step 170, loss 0.125186, acc 0.421875\n",
      "TRAIN 2018-07-26T17:20:25.496161: step 171, loss 0.125298, acc 0.4375\n",
      "TRAIN 2018-07-26T17:20:25.687537: step 172, loss 0.124656, acc 0.59375\n",
      "TRAIN 2018-07-26T17:20:25.871897: step 173, loss 0.126193, acc 0.328125\n",
      "TRAIN 2018-07-26T17:20:26.092103: step 174, loss 0.124873, acc 0.5625\n",
      "TRAIN 2018-07-26T17:20:26.287639: step 175, loss 0.125015, acc 0.453125\n",
      "TRAIN 2018-07-26T17:20:26.487196: step 176, loss 0.125106, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:26.732112: step 177, loss 0.124624, acc 0.578125\n",
      "TRAIN 2018-07-26T17:20:26.945143: step 178, loss 0.126096, acc 0.390625\n",
      "TRAIN 2018-07-26T17:20:27.179463: step 179, loss 0.124489, acc 0.5625\n",
      "TRAIN 2018-07-26T17:20:27.379469: step 180, loss 0.125214, acc 0.484375\n",
      "TRAIN 2018-07-26T17:20:27.574507: step 181, loss 0.12621, acc 0.390625\n",
      "TRAIN 2018-07-26T17:20:27.770370: step 182, loss 0.12538, acc 0.4375\n",
      "TRAIN 2018-07-26T17:20:27.971532: step 183, loss 0.125039, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:28.191501: step 184, loss 0.125213, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:28.473574: step 185, loss 0.12447, acc 0.5625\n",
      "TRAIN 2018-07-26T17:20:28.674836: step 186, loss 0.125531, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:28.888820: step 187, loss 0.127691, acc 0.34375\n",
      "TRAIN 2018-07-26T17:20:29.077900: step 188, loss 0.124885, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:29.287574: step 189, loss 0.124788, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:29.487359: step 190, loss 0.124855, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:29.691394: step 191, loss 0.124778, acc 0.5625\n",
      "TRAIN 2018-07-26T17:20:29.886657: step 192, loss 0.124864, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:30.077391: step 193, loss 0.125438, acc 0.4375\n",
      "TRAIN 2018-07-26T17:20:30.289113: step 194, loss 0.124927, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:30.489256: step 195, loss 0.124766, acc 0.546875\n",
      "TRAIN 2018-07-26T17:20:30.688625: step 196, loss 0.124514, acc 0.578125\n",
      "TRAIN 2018-07-26T17:20:30.888553: step 197, loss 0.125215, acc 0.484375\n",
      "TRAIN 2018-07-26T17:20:31.088220: step 198, loss 0.125682, acc 0.453125\n",
      "TRAIN 2018-07-26T17:20:31.291111: step 199, loss 0.123691, acc 0.609375\n",
      "TRAIN 2018-07-26T17:20:31.531392: step 200, loss 0.125645, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:31.740645: step 201, loss 0.124324, acc 0.546875\n",
      "TRAIN 2018-07-26T17:20:31.989666: step 202, loss 0.126467, acc 0.4375\n",
      "TRAIN 2018-07-26T17:20:32.293745: step 203, loss 0.123985, acc 0.5625\n",
      "TRAIN 2018-07-26T17:20:32.489067: step 204, loss 0.126755, acc 0.421875\n",
      "TRAIN 2018-07-26T17:20:32.683338: step 205, loss 0.124354, acc 0.546875\n",
      "TRAIN 2018-07-26T17:20:32.873533: step 206, loss 0.126299, acc 0.421875\n",
      "TRAIN 2018-07-26T17:20:33.087079: step 207, loss 0.125998, acc 0.40625\n",
      "TRAIN 2018-07-26T17:20:33.285906: step 208, loss 0.124899, acc 0.5625\n",
      "TRAIN 2018-07-26T17:20:33.499891: step 209, loss 0.124664, acc 0.59375\n",
      "TRAIN 2018-07-26T17:20:33.702586: step 210, loss 0.124335, acc 0.5625\n",
      "TRAIN 2018-07-26T17:20:33.903899: step 211, loss 0.124892, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:34.104094: step 212, loss 0.122712, acc 0.59375\n",
      "TRAIN 2018-07-26T17:20:34.300667: step 213, loss 0.123938, acc 0.546875\n",
      "TRAIN 2018-07-26T17:20:34.529399: step 214, loss 0.127587, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:34.719083: step 215, loss 0.127995, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:34.906168: step 216, loss 0.124764, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:35.104848: step 217, loss 0.122232, acc 0.578125\n",
      "TRAIN 2018-07-26T17:20:35.317516: step 218, loss 0.128287, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:35.557702: step 219, loss 0.132332, acc 0.390625\n",
      "TRAIN 2018-07-26T17:20:35.784339: step 220, loss 0.128996, acc 0.4375\n",
      "TRAIN 2018-07-26T17:20:36.004999: step 221, loss 0.123386, acc 0.5625\n",
      "TRAIN 2018-07-26T17:20:36.264066: step 222, loss 0.126284, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:36.468213: step 223, loss 0.126071, acc 0.453125\n",
      "TRAIN 2018-07-26T17:20:36.674161: step 224, loss 0.125036, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:36.870088: step 225, loss 0.124924, acc 0.5625\n",
      "TRAIN 2018-07-26T17:20:37.077732: step 226, loss 0.125824, acc 0.4375\n",
      "TRAIN 2018-07-26T17:20:37.272698: step 227, loss 0.124884, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:37.517948: step 228, loss 0.126895, acc 0.4375\n",
      "TRAIN 2018-07-26T17:20:37.741585: step 229, loss 0.129251, acc 0.359375\n",
      "TRAIN 2018-07-26T17:20:37.936284: step 230, loss 0.126737, acc 0.4375\n",
      "TRAIN 2018-07-26T17:20:38.161865: step 231, loss 0.125137, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:38.354051: step 232, loss 0.125949, acc 0.40625\n",
      "TRAIN 2018-07-26T17:20:38.569807: step 233, loss 0.125043, acc 0.421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN 2018-07-26T17:20:38.761051: step 234, loss 0.125128, acc 0.484375\n",
      "TRAIN 2018-07-26T17:20:39.029704: step 235, loss 0.124709, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:39.315779: step 236, loss 0.123378, acc 0.609375\n",
      "TRAIN 2018-07-26T17:20:39.519860: step 237, loss 0.126457, acc 0.453125\n",
      "TRAIN 2018-07-26T17:20:39.713877: step 238, loss 0.124968, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:39.914335: step 239, loss 0.126039, acc 0.484375\n",
      "TRAIN 2018-07-26T17:20:40.117273: step 240, loss 0.126084, acc 0.484375\n",
      "TRAIN 2018-07-26T17:20:40.328538: step 241, loss 0.125525, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:40.540045: step 242, loss 0.123098, acc 0.578125\n",
      "TRAIN 2018-07-26T17:20:40.727205: step 243, loss 0.124512, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:41.023562: step 244, loss 0.124982, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:41.286631: step 245, loss 0.12359, acc 0.5625\n",
      "TRAIN 2018-07-26T17:20:41.546005: step 246, loss 0.125931, acc 0.484375\n",
      "TRAIN 2018-07-26T17:20:41.771611: step 247, loss 0.12314, acc 0.578125\n",
      "TRAIN 2018-07-26T17:20:42.041681: step 248, loss 0.128253, acc 0.40625\n",
      "TRAIN 2018-07-26T17:20:42.257278: step 249, loss 0.125355, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:42.455854: step 250, loss 0.124903, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:42.670579: step 251, loss 0.123108, acc 0.609375\n",
      "TRAIN 2018-07-26T17:20:42.898852: step 252, loss 0.126914, acc 0.40625\n",
      "TRAIN 2018-07-26T17:20:43.123119: step 253, loss 0.122817, acc 0.65625\n",
      "TRAIN 2018-07-26T17:20:43.364950: step 254, loss 0.126349, acc 0.421875\n",
      "TRAIN 2018-07-26T17:20:43.639020: step 255, loss 0.126162, acc 0.421875\n",
      "TRAIN 2018-07-26T17:20:43.907091: step 256, loss 0.125885, acc 0.40625\n",
      "TRAIN 2018-07-26T17:20:44.178160: step 257, loss 0.124951, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:44.451231: step 258, loss 0.1252, acc 0.453125\n",
      "TRAIN 2018-07-26T17:20:44.642333: step 259, loss 0.125031, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:44.908324: step 260, loss 0.123747, acc 0.625\n",
      "TRAIN 2018-07-26T17:20:45.171395: step 261, loss 0.124879, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:45.432460: step 262, loss 0.12525, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:45.649782: step 263, loss 0.124522, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:45.824550: step 264, loss 0.124031, acc 0.546875\n",
      "TRAIN 2018-07-26T17:20:46.025933: step 265, loss 0.124518, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:46.219827: step 266, loss 0.123333, acc 0.5625\n",
      "TRAIN 2018-07-26T17:20:46.432578: step 267, loss 0.125913, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:46.620287: step 268, loss 0.130224, acc 0.40625\n",
      "TRAIN 2018-07-26T17:20:46.818749: step 269, loss 0.129172, acc 0.421875\n",
      "TRAIN 2018-07-26T17:20:47.026326: step 270, loss 0.125621, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:47.219763: step 271, loss 0.125818, acc 0.484375\n",
      "TRAIN 2018-07-26T17:20:47.420837: step 272, loss 0.127298, acc 0.390625\n",
      "TRAIN 2018-07-26T17:20:47.615638: step 273, loss 0.125126, acc 0.484375\n",
      "TRAIN 2018-07-26T17:20:47.841930: step 274, loss 0.125392, acc 0.4375\n",
      "TRAIN 2018-07-26T17:20:48.035186: step 275, loss 0.124655, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:48.283411: step 276, loss 0.124902, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:48.508130: step 277, loss 0.124513, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:48.686839: step 278, loss 0.123391, acc 0.5625\n",
      "TRAIN 2018-07-26T17:20:48.872499: step 279, loss 0.123907, acc 0.546875\n",
      "TRAIN 2018-07-26T17:20:49.075625: step 280, loss 0.125481, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:49.268017: step 281, loss 0.12223, acc 0.578125\n",
      "TRAIN 2018-07-26T17:20:49.481306: step 282, loss 0.130482, acc 0.4375\n",
      "TRAIN 2018-07-26T17:20:49.701524: step 283, loss 0.122178, acc 0.578125\n",
      "TRAIN 2018-07-26T17:20:49.909375: step 284, loss 0.130013, acc 0.4375\n",
      "TRAIN 2018-07-26T17:20:50.166694: step 285, loss 0.122375, acc 0.578125\n",
      "TRAIN 2018-07-26T17:20:50.353689: step 286, loss 0.126706, acc 0.484375\n",
      "TRAIN 2018-07-26T17:20:50.565643: step 287, loss 0.127622, acc 0.453125\n",
      "TRAIN 2018-07-26T17:20:50.753702: step 288, loss 0.125524, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:50.954744: step 289, loss 0.125338, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:51.136566: step 290, loss 0.128386, acc 0.34375\n",
      "TRAIN 2018-07-26T17:20:51.338207: step 291, loss 0.124714, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:51.562422: step 292, loss 0.125008, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:51.772701: step 293, loss 0.125036, acc 0.484375\n",
      "TRAIN 2018-07-26T17:20:51.973557: step 294, loss 0.125126, acc 0.484375\n",
      "TRAIN 2018-07-26T17:20:52.184055: step 295, loss 0.12593, acc 0.40625\n",
      "TRAIN 2018-07-26T17:20:52.374534: step 296, loss 0.125342, acc 0.453125\n",
      "TRAIN 2018-07-26T17:20:52.585713: step 297, loss 0.12499, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:52.825776: step 298, loss 0.125165, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:53.018292: step 299, loss 0.124304, acc 0.59375\n",
      "TRAIN 2018-07-26T17:20:53.219886: step 300, loss 0.124604, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:53.420945: step 301, loss 0.125408, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:53.663432: step 302, loss 0.12396, acc 0.546875\n",
      "TRAIN 2018-07-26T17:20:53.889461: step 303, loss 0.125291, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:54.082471: step 304, loss 0.12769, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:54.288313: step 305, loss 0.127893, acc 0.453125\n",
      "TRAIN 2018-07-26T17:20:54.486880: step 306, loss 0.128768, acc 0.375\n",
      "TRAIN 2018-07-26T17:20:54.686699: step 307, loss 0.12502, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:54.886364: step 308, loss 0.123555, acc 0.578125\n",
      "TRAIN 2018-07-26T17:20:55.118618: step 309, loss 0.125159, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:55.319326: step 310, loss 0.124682, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:55.520798: step 311, loss 0.126488, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:55.726905: step 312, loss 0.128144, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:55.921150: step 313, loss 0.12538, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:56.146623: step 314, loss 0.125692, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:56.338965: step 315, loss 0.127123, acc 0.421875\n",
      "TRAIN 2018-07-26T17:20:56.537584: step 316, loss 0.124986, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:56.750647: step 317, loss 0.124887, acc 0.515625\n",
      "TRAIN 2018-07-26T17:20:56.943567: step 318, loss 0.123322, acc 0.5625\n",
      "TRAIN 2018-07-26T17:20:57.210840: step 319, loss 0.124924, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:57.434037: step 320, loss 0.121954, acc 0.578125\n",
      "TRAIN 2018-07-26T17:20:57.620641: step 321, loss 0.1331, acc 0.453125\n",
      "TRAIN 2018-07-26T17:20:57.828849: step 322, loss 0.125733, acc 0.53125\n",
      "TRAIN 2018-07-26T17:20:58.018707: step 323, loss 0.124105, acc 0.546875\n",
      "TRAIN 2018-07-26T17:20:58.203738: step 324, loss 0.122262, acc 0.578125\n",
      "TRAIN 2018-07-26T17:20:58.407057: step 325, loss 0.128031, acc 0.453125\n",
      "TRAIN 2018-07-26T17:20:58.618766: step 326, loss 0.125371, acc 0.5\n",
      "TRAIN 2018-07-26T17:20:58.803014: step 327, loss 0.124547, acc 0.546875\n",
      "TRAIN 2018-07-26T17:20:59.031230: step 328, loss 0.12503, acc 0.40625\n",
      "TRAIN 2018-07-26T17:20:59.234580: step 329, loss 0.124414, acc 0.546875\n",
      "TRAIN 2018-07-26T17:20:59.475852: step 330, loss 0.126334, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:59.682785: step 331, loss 0.126876, acc 0.46875\n",
      "TRAIN 2018-07-26T17:20:59.876325: step 332, loss 0.12456, acc 0.53125\n",
      "TRAIN 2018-07-26T17:21:00.086817: step 333, loss 0.124588, acc 0.53125\n",
      "TRAIN 2018-07-26T17:21:00.311695: step 334, loss 0.124607, acc 0.53125\n",
      "TRAIN 2018-07-26T17:21:00.532905: step 335, loss 0.127466, acc 0.46875\n",
      "TRAIN 2018-07-26T17:21:00.726863: step 336, loss 0.123909, acc 0.546875\n",
      "TRAIN 2018-07-26T17:21:00.960812: step 337, loss 0.125828, acc 0.5\n",
      "TRAIN 2018-07-26T17:21:01.153893: step 338, loss 0.128002, acc 0.4375\n",
      "TRAIN 2018-07-26T17:21:01.348414: step 339, loss 0.126385, acc 0.46875\n",
      "TRAIN 2018-07-26T17:21:01.553827: step 340, loss 0.126552, acc 0.4375\n",
      "TRAIN 2018-07-26T17:21:01.738335: step 341, loss 0.124734, acc 0.53125\n",
      "TRAIN 2018-07-26T17:21:01.958400: step 342, loss 0.125065, acc 0.4375\n",
      "TRAIN 2018-07-26T17:21:02.158202: step 343, loss 0.125349, acc 0.46875\n",
      "TRAIN 2018-07-26T17:21:02.353875: step 344, loss 0.127085, acc 0.390625\n",
      "TRAIN 2018-07-26T17:21:02.538318: step 345, loss 0.127649, acc 0.375\n",
      "TRAIN 2018-07-26T17:21:02.736776: step 346, loss 0.126116, acc 0.4375\n",
      "TRAIN 2018-07-26T17:21:02.921763: step 347, loss 0.125949, acc 0.40625\n",
      "TRAIN 2018-07-26T17:21:03.123150: step 348, loss 0.124948, acc 0.59375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN 2018-07-26T17:21:03.322725: step 349, loss 0.12475, acc 0.5625\n",
      "TRAIN 2018-07-26T17:21:03.521018: step 350, loss 0.123783, acc 0.625\n",
      "TRAIN 2018-07-26T17:21:03.731630: step 351, loss 0.126369, acc 0.4375\n",
      "TRAIN 2018-07-26T17:21:03.910836: step 352, loss 0.124536, acc 0.53125\n",
      "TRAIN 2018-07-26T17:21:04.142689: step 353, loss 0.125873, acc 0.484375\n",
      "TRAIN 2018-07-26T17:21:04.372152: step 354, loss 0.122523, acc 0.59375\n",
      "TRAIN 2018-07-26T17:21:04.583461: step 355, loss 0.125087, acc 0.515625\n",
      "TRAIN 2018-07-26T17:21:04.793500: step 356, loss 0.122703, acc 0.578125\n",
      "TRAIN 2018-07-26T17:21:05.000843: step 357, loss 0.125257, acc 0.515625\n",
      "TRAIN 2018-07-26T17:21:05.196645: step 358, loss 0.12247, acc 0.578125\n",
      "TRAIN 2018-07-26T17:21:05.404411: step 359, loss 0.124671, acc 0.53125\n",
      "TRAIN 2018-07-26T17:21:05.608954: step 360, loss 0.129541, acc 0.4375\n",
      "TRAIN 2018-07-26T17:21:05.822945: step 361, loss 0.127839, acc 0.46875\n",
      "TRAIN 2018-07-26T17:21:06.026795: step 362, loss 0.122433, acc 0.578125\n",
      "TRAIN 2018-07-26T17:21:06.225773: step 363, loss 0.128808, acc 0.4375\n",
      "TRAIN 2018-07-26T17:21:06.430325: step 364, loss 0.123925, acc 0.546875\n",
      "TRAIN 2018-07-26T17:21:06.633110: step 365, loss 0.126758, acc 0.46875\n",
      "TRAIN 2018-07-26T17:21:06.826393: step 366, loss 0.122624, acc 0.59375\n",
      "TRAIN 2018-07-26T17:21:07.058395: step 367, loss 0.123679, acc 0.5625\n",
      "TRAIN 2018-07-26T17:21:07.262271: step 368, loss 0.12843, acc 0.375\n",
      "TRAIN 2018-07-26T17:21:07.452946: step 369, loss 0.123982, acc 0.5625\n",
      "TRAIN 2018-07-26T17:21:07.655860: step 370, loss 0.124878, acc 0.515625\n",
      "TRAIN 2018-07-26T17:21:07.871786: step 371, loss 0.125252, acc 0.484375\n",
      "TRAIN 2018-07-26T17:21:08.079473: step 372, loss 0.124908, acc 0.515625\n",
      "TRAIN 2018-07-26T17:21:08.293408: step 373, loss 0.125869, acc 0.3125\n",
      "TRAIN 2018-07-26T17:21:08.469844: step 374, loss 0.125101, acc 0.46875\n",
      "TRAIN 2018-07-26T17:21:08.668982: step 375, loss 0.124899, acc 0.515625\n",
      "TRAIN 2018-07-26T17:21:08.855527: step 376, loss 0.124421, acc 0.546875\n",
      "TRAIN 2018-07-26T17:21:09.044996: step 377, loss 0.125527, acc 0.484375\n",
      "TRAIN 2018-07-26T17:21:09.235500: step 378, loss 0.122991, acc 0.59375\n",
      "TRAIN 2018-07-26T17:21:09.420847: step 379, loss 0.125926, acc 0.484375\n"
     ]
    }
   ],
   "source": [
    "batches=inpH.batch_iter(\n",
    "            list(zip(train_set[0], train_set[1], train_set[2])), batch_size, num_epochs)\n",
    "\n",
    "ptr=0\n",
    "max_validation_acc=0.0\n",
    "for nn in range(sum_no_of_batches*num_epochs):\n",
    "    batch = batches.__next__()\n",
    "\n",
    "    if len(batch)<1:\n",
    "        continue\n",
    "    x1_batch,x2_batch, y_batch = zip(*batch)\n",
    "\n",
    "    if len(y_batch)<1:\n",
    "        continue\n",
    "    #print(x1_batch.shape)\n",
    "    #print(x2_batch.shape)\n",
    "    train_step(x1_batch, x2_batch, y_batch)\n",
    "    current_step = tf.train.global_step(sess, global_step)\n",
    "    sum_acc=0.0\n",
    "    if current_step % evaluate_every == 0:\n",
    "        print(\"\\nEvaluation:\")\n",
    "        dev_batches = inpH.batch_iter(list(zip(dev_set[0],dev_set[1],dev_set[2])), batch_size, 1)\n",
    "        for db in dev_batches:\n",
    "            if len(db)<1:\n",
    "                continue\n",
    "            x1_dev_b,x2_dev_b,y_dev_b = zip(*db)\n",
    "            if len(y_dev_b)<1:\n",
    "                continue\n",
    "            acc = dev_step(x1_dev_b, x2_dev_b, y_dev_b)\n",
    "            sum_acc = sum_acc + acc\n",
    "        print(\"\")\n",
    "    if current_step % checkpoint_every == 0:\n",
    "        if sum_acc >= max_validation_acc:\n",
    "            max_validation_acc = sum_acc\n",
    "            saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "            tf.train.write_graph(sess.graph.as_graph_def(), checkpoint_prefix, \"graph\"+str(nn)+\".pb\", as_text=False)\n",
    "            print(\"Saved model {} with sum_accuracy={} checkpoint to {}\\n\".format(nn, max_validation_acc, checkpoint_prefix))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
