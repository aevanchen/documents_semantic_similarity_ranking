{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import random\n",
    "random.seed(49999)\n",
    "import numpy\n",
    "\n",
    "sys.path.append('../matchzoo/inputs/')\n",
    "sys.path.append('../matchzoo/utils/')\n",
    "from preparation import Preparation\n",
    "from preprocess import Preprocess, NgramUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    prepare = Preparation()\n",
    "    srcdir = '../matchzoo/'\n",
    "    dstdir = 'D:/nlp_data/'\n",
    "\n",
    "\n",
    "    ####################\n",
    "    #input is quora data\n",
    "    #infile = srcdir + 'quora_duplicate_questions.tsv'\n",
    "    #corpus, rels = prepare.run_with_one_corpus_for_quora(infile)\n",
    "\n",
    "\n",
    "    #####################\n",
    "    #input is SICK data\n",
    "    infile = srcdir + 'train_snli.txt'\n",
    "\n",
    "    order=2\n",
    "    #order=2ï¼Œ sen1,sen2, 0 or 1\n",
    "    #order=1.  0 or 1, sen1, sen2\n",
    "    corpus, rels = prepare.run_with_one_corpus(infile,order)\n",
    "    print('total corpus : %d ...' % (len(corpus)))\n",
    "    print('total relations : %d ...' % (len(rels)))\n",
    "    prepare.save_corpus(dstdir + 'corpus.txt', corpus)\n",
    "    prepare.save_relation(dstdir + 'relation.txt', rels)\n",
    "\n",
    "    rel_train, rel_valid, rel_test = prepare.split_train_valid_test(rels, [0.8, 0.1, 0.1])\n",
    "    prepare.save_relation(dstdir + 'relation_train.txt', rel_train)\n",
    "    prepare.save_relation(dstdir + 'relation_valid.txt', rel_valid)\n",
    "    prepare.save_relation(dstdir + 'relation_test.txt', rel_test)\n",
    "    print('Preparation finished ...')\n",
    "\n",
    "    #filter output stop words\n",
    "    preprocessor = Preprocess(word_stem_config={'enable': False})\n",
    "    #preprocessor = Preprocess(word_stem_config={'enable': False},word_filter_config={'enable':False})\n",
    "\n",
    "\n",
    "    dids, docs = preprocessor.run(dstdir + 'corpus.txt')\n",
    "    preprocessor.save_word_dict(dstdir + 'word_dict.txt')\n",
    "    preprocessor.save_words_stats(dstdir + 'word_stats.txt')\n",
    "    fout = open(dstdir + 'corpus_preprocessed.txt', 'w')\n",
    "    for inum, did in enumerate(dids):\n",
    "        fout.write('%s\\t%s\\n' % (did, ' '.join(map(str, docs[inum]))))\n",
    "    fout.close()\n",
    "    print('preprocess finished ...')\n",
    "    print('preprocess finished ...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word embedding\n",
    "#w2v_file = sys.argv[1]  # w2v_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf8\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import six\n",
    "import io\n",
    "import array\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "\n",
    "def load_word_dict(word_map_file):\n",
    "    \"\"\" file -> {word: index} \"\"\"\n",
    "    word_dict = {}\n",
    "    for line in tqdm(io.open(word_map_file, encoding='utf8')):\n",
    "        line = line.split()\n",
    "        try:\n",
    "            word_dict[line[0]] = int(line[1])\n",
    "        except:\n",
    "            print(line)\n",
    "            continue\n",
    "    return word_dict\n",
    "def load_word_embedding(vocab, w2v_file, file_format):\n",
    "    \"\"\"\n",
    "    Pros:\n",
    "        Save the oov words in oov.p for further analysis.\n",
    "    Refs:\n",
    "        class Vectors, https://github.com/pytorch/text/blob/master/torchtext/vocab.py\n",
    "    Args:\n",
    "        vocab: dict,\n",
    "        w2v_file: file, path to file of pre-trained word2vec/glove/fasttext\n",
    "    Returns:\n",
    "        vectors\n",
    "    \"\"\"\n",
    "    if (file_format=='text'):\n",
    "        pre_trained = {}\n",
    "        n_words = len(vocab)\n",
    "        embeddings = None  # (n_words, n_dim)\n",
    "\n",
    "        # str call is necessary for Python 2/3 compatibility, since\n",
    "        # argument must be Python 2 str (Python 3 bytes) or\n",
    "        # Python 3 str (Python 2 unicode)\n",
    "        vectors, dim = array.array(str('d')), None\n",
    "\n",
    "        # Try to read the whole file with utf-8 encoding.\n",
    "        binary_lines = False\n",
    "        try:\n",
    "            with io.open(w2v_file, encoding=\"utf8\") as f:\n",
    "                lines = [line for line in f]\n",
    "        # If there are malformed lines, read in binary mode\n",
    "        # and manually decode each word from utf-8\n",
    "        except:\n",
    "            print(\"Could not read {} as UTF8 file, \"\n",
    "                  \"reading file as bytes and skipping \"\n",
    "                  \"words with malformed UTF8.\".format(w2v_file))\n",
    "            with open(w2v_file, 'rb') as f:\n",
    "                lines = [line for line in f]\n",
    "            binary_lines = True\n",
    "\n",
    "        print(\"Loading vectors from {}\".format(w2v_file))\n",
    "\n",
    "        for line in tqdm(lines):\n",
    "            # Explicitly splitting on \" \" is important, so we don't\n",
    "            # get rid of Unicode non-breaking spaces in the vectors.\n",
    "            entries = line.rstrip().split(b\" \" if binary_lines else \" \")\n",
    "\n",
    "            word, entries = entries[0], entries[1:]\n",
    "            if dim is None and len(entries) > 1:\n",
    "                dim = len(entries)\n",
    "                # init the embeddings\n",
    "                embeddings = np.random.uniform(-0.25, 0.25, (n_words, dim))\n",
    "\n",
    "            elif len(entries) == 1:\n",
    "                print(\"Skipping token {} with 1-dimensional \"\n",
    "                      \"vector {}; likely a header\".format(word, entries))\n",
    "                continue\n",
    "            elif dim != len(entries):\n",
    "                raise RuntimeError(\n",
    "                    \"Vector for token {} has {} dimensions, but previously \"\n",
    "                    \"read vectors have {} dimensions. All vectors must have \"\n",
    "                    \"the same number of dimensions.\".format(word, len(entries), dim))\n",
    "\n",
    "            if binary_lines:\n",
    "                try:\n",
    "                    if isinstance(word, six.binary_type):\n",
    "                        word = word.decode('utf-8')\n",
    "\n",
    "                except:\n",
    "                    print(\"Skipping non-UTF8 token {}\".format(repr(word)))\n",
    "                    continue\n",
    "\n",
    "            if word in vocab and word not in pre_trained:\n",
    "                embeddings[vocab[word]] = [float(x) for x in entries]\n",
    "                pre_trained[word] = 1\n",
    "\n",
    "        # init tht OOV word embeddings\n",
    "        for word in vocab:\n",
    "            if word not in pre_trained:\n",
    "                alpha = 0.5 * (2.0 * np.random.random() - 1.0)\n",
    "                curr_embed = (2.0 * np.random.random_sample([dim]) - 1.0) * alpha\n",
    "                embeddings[vocab[word]] = curr_embed\n",
    "\n",
    "        pre_trained_len = len(pre_trained)\n",
    "        print('Pre-trained: {}/{} {:.2f}'.format(pre_trained_len, n_words, pre_trained_len * 100.0 / n_words))\n",
    "\n",
    "        oov_word_list = [w for w in vocab if w not in pre_trained]\n",
    "        print('oov word list example (30): ', oov_word_list[:30])\n",
    "        pickle.dump(oov_word_list, open('oov.p', 'wb'), protocol=2)\n",
    "\n",
    "        embeddings = np.array(embeddings, dtype=np.float32)\n",
    "        return embeddings\n",
    "    elif (file_format=='bin'):\n",
    "        dim=w2v_file['the'].shape[0]\n",
    "        print(dim)\n",
    "        #pre_trained = {}\n",
    "        n_words = len(vocab)\n",
    "        print(n_words)\n",
    "        embeddings = np.random.uniform(-0.25, 0.25, (n_words, dim))\n",
    "        for word in vocab:\n",
    "            t=vocab[word]\n",
    "            try:\n",
    "                embeddings[t]=model[word]\n",
    "            except:\n",
    "                alpha = 0.5 * (2.0 * np.random.random() - 1.0)\n",
    "                curr_embed = (2.0 * np.random.random_sample([dim]) - 1.0) * alpha\n",
    "                embeddings[t] = curr_embed\n",
    "                \n",
    "        embeddings = np.array(embeddings, dtype=np.float32)\n",
    "        return embeddings\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word embedding\n",
    "#w2v_file = sys.argv[1]  # w2v_file\n",
    "w2v_file = \"D:/simple_vec/wiki.simple.bin\"  # w2v_file\n",
    "#word_dict_file = sys.argv[2]  # word_dict_file\n",
    "word_dict_file=\"D:/nlp_data/word_dict.txt\"\n",
    "#mapped_w2v_file = \"embed_fasttext_d300\" # output shared w2v dict\n",
    "\n",
    "\n",
    "print(\"Fasttext model loading....\")\n",
    "model=FastText.load_fasttext_format(w2v_file )\n",
    "print(\"Model Loaded\")\n",
    "\n",
    "word_dict = {}\n",
    "print('load word dict ...')\n",
    "word_dict = load_word_dict(word_dict_file)\n",
    "print('load word vectors ...')\n",
    "file_format='bin'\n",
    "embeddings = load_word_embedding(word_dict, model,file_format)\n",
    "mapped_w2v_file = \"embed_fasttext_d300\" # output shared w2v dict\n",
    "print('save word vectors ...')\n",
    "with open(mapped_w2v_file, 'w') as fw:\n",
    "    # assert word_dict\n",
    "    for w, idx in tqdm(word_dict.items()):\n",
    "        print(word_dict[w], ' '.join(map(str, embeddings[idx])), file=fw)\n",
    "\n",
    "print('Map word vectors finished ...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf8\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import numpy\n",
    "import math\n",
    "\n",
    "infile = mapped_w2v_file\n",
    "\n",
    "outfile = \"embed_fasttext_d300_norm\"\n",
    "fout = codecs.open(outfile,'w', encoding='utf8')\n",
    "with codecs.open(infile, 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        r = line.split()\n",
    "        w = r[0]\n",
    "        try:\n",
    "            # BUG: it will happen `name@domain.com`\n",
    "            vec = [float(k) for k in r[1:]]\n",
    "        except:\n",
    "            print(line)\n",
    "        sum = 0.0\n",
    "        for k in vec:\n",
    "            sum += k * k\n",
    "        sum = math.sqrt(sum)\n",
    "        for i,k in enumerate(vec):\n",
    "            vec[i] /= sum\n",
    "        print(w, ' '.join(['%f' % k for k in vec]), file=fout)\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\"\"\"\n",
    "Generate bin sum used in the attention based neural matching model (aNMM)\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../../matchzoo/utils/')\n",
    "sys.path.append('../../matchzoo/inputs/')\n",
    "from preprocess import cal_binsum\n",
    "from rank_io import *\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    bin_num = 100\n",
    "    srcdir = './'\n",
    "    embedfile = srcdir + 'embed_fasttext_d300_norm'\n",
    "    corpusfile =  'D:/nlp_data/corpus_preprocessed.txt'\n",
    "\n",
    "    relfiles = ['D:/nlp_data/relation_train.txt',\n",
    "           'D:/nlp_data/relation_valid.txt',\n",
    "            'D:/nlp_data/relation_test.txt'\n",
    "            ]\n",
    "    binfiles = [\n",
    "            srcdir + 'relation_train.binsum-%d.txt' % bin_num,\n",
    "            srcdir + 'relation_valid.binsum-%d.txt' % bin_num,\n",
    "            srcdir + 'relation_test.binsum-%d.txt' % bin_num\n",
    "            ]\n",
    "    embed_dict = read_embedding(filename = embedfile)\n",
    "    print('read embedding finished ...')\n",
    "    _PAD_ = len(embed_dict)\n",
    "    embed_size = len(list(embed_dict.values())[0])\n",
    "    embed_dict[_PAD_] = np.zeros((embed_size, ), dtype=np.float32)\n",
    "    embed = np.float32(np.random.uniform(-0.2, 0.2, [_PAD_+1, embed_size]))\n",
    "    embed = convert_embed_2_numpy(embed_dict, embed = embed)\n",
    "\n",
    "    corpus, _ = read_data(corpusfile)\n",
    "    print('read corpus finished....')\n",
    "    for idx, relfile in enumerate(relfiles):\n",
    "        binfile = binfiles[idx]\n",
    "        rel = read_relation(relfile)\n",
    "        fout = open(binfile, 'w')\n",
    "        for label, d1, d2 in rel:\n",
    "            assert d1 in corpus\n",
    "            assert d2 in corpus\n",
    "            qnum = len(corpus[d1])\n",
    "            d1_embed = embed[corpus[d1]]\n",
    "            d2_embed = embed[corpus[d2]]\n",
    "            curr_bin_sum = cal_binsum(d1_embed, d2_embed, qnum, bin_num)\n",
    "            curr_bin_sum = curr_bin_sum.tolist()\n",
    "            fout.write(' '.join(map(str, curr_bin_sum)))\n",
    "            fout.write('\\n')\n",
    "        fout.close()\n",
    "    print('generate bin sum finished ...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filtering import normalizeDocument\n",
    "\n",
    "normalizeDocument(\"i love you're perfect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "normalizeDocument(\"i love you're perfect. this is ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=float('nan') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filtering import normalizeDocument\n",
    "text=\"man32 is better\"\n",
    "normalizeDocument(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import random\n",
    "random.seed(49999)\n",
    "import numpy\n",
    "\n",
    "sys.path.append('../matchzoo/inputs/')\n",
    "sys.path.append('../matchzoo/utils/')\n",
    "from preparation import Preparation\n",
    "from preprocess import Preprocess, NgramUtil\n",
    "dstdir = 'D:/nlp_data/'\n",
    "\n",
    "preprocessor = Preprocess(word_stem_config={'enable': False})\n",
    "dids, docs = preprocessor.run(dstdir + 'corpus1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.save_word_dict(dstdir + 'word_dict1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['is']+model['better']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model['1']\n",
    "\n",
    "except:\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../matchzoo/utils/')\n",
    "sys.path.append('../matchzoo/inputs/')\n",
    "from preprocess import cal_binsum\n",
    "from rank_io import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'txt'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.split('.')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_file=\"word_dict1.txt\"\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'findall'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3293df2617a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'findall'"
     ]
    }
   ],
   "source": [
    "a.findall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'findLast'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-abbaf535a845>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindLast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'findLast'"
     ]
    }
   ],
   "source": [
    "a[a.findLast('.')+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_format=str(w2v_file)[(str(w2v_file)).find('.')+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'txt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
