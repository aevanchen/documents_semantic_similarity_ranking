{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import argparse\n",
    "import random\n",
    "random.seed(49999)\n",
    "import numpy as np\n",
    "random.seed(49999)\n",
    "import tensorflow\n",
    "tensorflow.set_random_seed(49999)\n",
    "from collections import OrderedDict\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "sys.path.append('/home/xingyuchen/jupyter/MatchZoo-master/matchzoo/utils/')\n",
    "sys.path.append('/home/xingyuchen/jupyter/MatchZoo-master/matchzoo/inputs/')\n",
    "from preprocess import *\n",
    "from rank_io import *\n",
    "from utils import *\n",
    "import inputs\n",
    "import metrics\n",
    "from losses import *\n",
    "from optimizers import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tensorflow.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tensorflow.Session(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "model_file =  \"/home/xingyuchen/jupyter/MatchZoo-master/matchzoo/anmm_classify.config\"\n",
    "with open(model_file, 'r') as f:\n",
    "    config = json.load(f)\n",
    "global_conf = config[\"global\"]\n",
    "optimizer = global_conf['optimizer']\n",
    "optimizer=optimizers.get(optimizer)\n",
    "K.set_value(optimizer.lr, 0.001)\n",
    "weights_file = str(global_conf['weights_file']) + '.%d'\n",
    "display_interval = int(global_conf['display_interval'])\n",
    "num_iters = int(global_conf['num_iters'])\n",
    "save_weights_iters = int(global_conf['save_weights_iters'])\n",
    "test_weights_iters=int(global_conf['test_weights_iters'])\n",
    "# read input config\n",
    "input_conf = config['inputs']\n",
    "share_input_conf = input_conf['share']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[embed_glove_d300_norm]\n",
      "\tEmbedding size: 28162\n",
      "Generate numpy embed: (28162, 300)\n",
      "[Embedding] Embedding Load Done.\n"
     ]
    }
   ],
   "source": [
    "# collect embedding\n",
    "\n",
    "embed_dict = read_embedding(filename=share_input_conf['embed_path'])\n",
    "_PAD_ = share_input_conf['vocab_size'] - 1\n",
    "embed_dict[_PAD_] = np.zeros((share_input_conf['embed_size'], ), dtype=np.float32)\n",
    "embed = np.float32(np.random.uniform(-0.02, 0.02, [share_input_conf['vocab_size'], share_input_conf['embed_size']]))\n",
    "share_input_conf['embed'] = convert_embed_2_numpy(embed_dict, embed = embed)\n",
    "print('[Embedding] Embedding Load Done.', end='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Input] Process Input Tags. odict_keys(['train']) in TRAIN, odict_keys(['valid', 'test']) in EVAL.\n",
      "[corpus_preprocessed.txt]\n",
      "\tData size: 537920\n",
      "[Dataset] 1 Dataset Load Done.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text1_corpus': 'corpus_preprocessed.txt', 'text2_corpus': 'corpus_preprocessed.txt', 'embed_path': 'embed_glove_d300_norm', 'use_dpool': False, 'embed_size': 300, 'vocab_size': 28162, 'train_embed': False, 'class_num': 2, 'target_mode': 'classification', 'bin_num': 100, 'text1_maxlen': 3, 'text2_maxlen': 100, 'embed': array([[ 0.036746,  0.103572, -0.007838, ...,  0.108598, -0.063425,\n",
      "         0.045992],\n",
      "       [ 0.022771, -0.013996, -0.033078, ...,  0.045675,  0.047962,\n",
      "        -0.028272],\n",
      "       [-0.027226, -0.01897 ,  0.042221, ..., -0.056826,  0.040489,\n",
      "         0.03461 ],\n",
      "       ...,\n",
      "       [ 0.101635,  0.109067,  0.029523, ..., -0.049203,  0.085788,\n",
      "        -0.036978],\n",
      "       [ 0.017096, -0.002005,  0.139227, ...,  0.046317, -0.02477 ,\n",
      "        -0.08832 ],\n",
      "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
      "         0.      ]], dtype=float32), 'input_type': 'DRMM_PointGenerator', 'phase': 'TRAIN', 'use_iter': False, 'query_per_iter': 20000, 'batch_per_iter': 5000, 'batch_size': 20, 'relation_file': 'relation_train.txt', 'hist_feats_file': 'relation_train.binsum-100.txt'}\n",
      "[relation_train.txt]\n",
      "\tInstance size: 323423\n",
      "[relation_train.binsum-100.txt]\n",
      "\tFeature size: 323423\n",
      "{'text1_corpus': 'corpus_preprocessed.txt', 'text2_corpus': 'corpus_preprocessed.txt', 'embed_path': 'embed_glove_d300_norm', 'use_dpool': False, 'embed_size': 300, 'vocab_size': 28162, 'train_embed': False, 'class_num': 2, 'target_mode': 'classification', 'bin_num': 100, 'text1_maxlen': 3, 'text2_maxlen': 100, 'embed': array([[ 0.036746,  0.103572, -0.007838, ...,  0.108598, -0.063425,\n",
      "         0.045992],\n",
      "       [ 0.022771, -0.013996, -0.033078, ...,  0.045675,  0.047962,\n",
      "        -0.028272],\n",
      "       [-0.027226, -0.01897 ,  0.042221, ..., -0.056826,  0.040489,\n",
      "         0.03461 ],\n",
      "       ...,\n",
      "       [ 0.101635,  0.109067,  0.029523, ..., -0.049203,  0.085788,\n",
      "        -0.036978],\n",
      "       [ 0.017096, -0.002005,  0.139227, ...,  0.046317, -0.02477 ,\n",
      "        -0.08832 ],\n",
      "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
      "         0.      ]], dtype=float32), 'input_type': 'DRMM_PointGenerator', 'phase': 'EVAL', 'batch_size': 10, 'relation_file': 'relation_valid.txt', 'hist_feats_file': 'relation_valid.binsum-100.txt'}\n",
      "[relation_valid.txt]\n",
      "\tInstance size: 40427\n",
      "[relation_valid.binsum-100.txt]\n",
      "\tFeature size: 40427\n",
      "{'text1_corpus': 'corpus_preprocessed.txt', 'text2_corpus': 'corpus_preprocessed.txt', 'embed_path': 'embed_glove_d300_norm', 'use_dpool': False, 'embed_size': 300, 'vocab_size': 28162, 'train_embed': False, 'class_num': 2, 'target_mode': 'classification', 'bin_num': 100, 'text1_maxlen': 3, 'text2_maxlen': 100, 'embed': array([[ 0.036746,  0.103572, -0.007838, ...,  0.108598, -0.063425,\n",
      "         0.045992],\n",
      "       [ 0.022771, -0.013996, -0.033078, ...,  0.045675,  0.047962,\n",
      "        -0.028272],\n",
      "       [-0.027226, -0.01897 ,  0.042221, ..., -0.056826,  0.040489,\n",
      "         0.03461 ],\n",
      "       ...,\n",
      "       [ 0.101635,  0.109067,  0.029523, ..., -0.049203,  0.085788,\n",
      "        -0.036978],\n",
      "       [ 0.017096, -0.002005,  0.139227, ...,  0.046317, -0.02477 ,\n",
      "        -0.08832 ],\n",
      "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
      "         0.      ]], dtype=float32), 'input_type': 'DRMM_PointGenerator', 'phase': 'EVAL', 'batch_size': 10, 'relation_file': 'relation_test.txt', 'hist_feats_file': 'relation_test.binsum-100.txt'}\n",
      "[relation_test.txt]\n",
      "\tInstance size: 40429\n",
      "[relation_test.binsum-100.txt]\n",
      "\tFeature size: 40429\n"
     ]
    }
   ],
   "source": [
    "# initial data generator\n",
    "train_gen = OrderedDict()\n",
    "eval_gen = OrderedDict()\n",
    "\n",
    "for tag, conf in input_train_conf.items():\n",
    "    print(conf, end='\\n')\n",
    "    conf['data1'] = dataset[conf['text1_corpus']]\n",
    "    conf['data2'] = dataset[conf['text2_corpus']]\n",
    "    generator = inputs.get(conf['input_type'])\n",
    "    train_gen[tag] = generator( config = conf )\n",
    "\n",
    "for tag, conf in input_eval_conf.items():\n",
    "    print(conf, end='\\n')\n",
    "    conf['data1'] = dataset[conf['text1_corpus']]\n",
    "    conf['data2'] = dataset[conf['text2_corpus']]\n",
    "    generator = inputs.get(conf['input_type'])\n",
    "    eval_gen[tag] = generator( config = conf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ANMM] init done\n",
      "[layer]: Input\t[shape]: [None, 3] \n",
      "84.4% memory has been used\n",
      "[layer]: Input\t[shape]: [None, 3, 100] \n",
      "84.4% memory has been used\n",
      "[layer]: Embedding\t[shape]: [None, 3, 300] \n",
      "84.6% memory has been used\n",
      "[layer]: Dense\t[shape]: [None, 3, 1] \n",
      "84.6% memory has been used\n",
      "[layer]: Lambda-softmax\t[shape]: [None, 3, 1] \n",
      "84.6% memory has been used\n",
      "[layer]: Dropout\t[shape]: [None, 3, 100] \n",
      "84.6% memory has been used\n",
      "[layer]: Dense\t[shape]: [None, 3, 50] \n",
      "84.6% memory has been used\n",
      "[layer]: Dense\t[shape]: [None, 3, 1] \n",
      "84.6% memory has been used\n",
      "[layer]: Permute\t[shape]: [None, 1, 3] \n",
      "84.6% memory has been used\n",
      "[layer]: Reshape\t[shape]: [None, 3] \n",
      "84.6% memory has been used\n",
      "[layer]: Reshape\t[shape]: [None, 3] \n",
      "84.6% memory has been used\n",
      "[layer]: Dense\t[shape]: [None, 2] \n",
      "84.6% memory has been used\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "sys.path.append('/home/xingyuchen/jupyter/MatchZoo-master/matchzoo/models/')\n",
    "from matchpyramid import *\n",
    "from anmm import *\n",
    "model_config = config['model']['setting']\n",
    "model_config.update(config['inputs']['share'])\n",
    "sys.path.insert(0, config['model']['model_path'])\n",
    "\n",
    "model= ANMM( model_config).build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] Model Compile Done.\n"
     ]
    }
   ],
   "source": [
    "loss = []\n",
    "for lobj in config['losses']:\n",
    "    if lobj['object_name'] in mz_specialized_losses:\n",
    "        loss.append(rank_losses.get(lobj['object_name'])(lobj['object_params']))\n",
    "    else:\n",
    "        loss.append(rank_losses.get(lobj['object_name']))\n",
    "eval_metrics = OrderedDict()\n",
    "for mobj in config['metrics']:\n",
    "    mobj = mobj.lower()\n",
    "    if '@' in mobj:\n",
    "        mt_key, mt_val = mobj.split('@', 1)\n",
    "        eval_metrics[mobj] = metrics.get(mt_key)(int(mt_val))\n",
    "    else:\n",
    "        eval_metrics[mobj] = metrics.get(mobj)\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "print('[Model] Model Compile Done.', end='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "      i=0\n",
    "      for tag, generator in eval_gen.items():\n",
    "        if(i==1):\n",
    "            continue\n",
    "        genfun = generator.get_batch_generator()\n",
    "        print('[%s]\\t[Eval:%s] ' % (time.strftime('%m-%d-%Y %H:%M:%S', time.localtime(time.time())), tag), end='')\n",
    "        res = dict([[k,0.] for k in eval_metrics.keys()])\n",
    "        num_valid = 0\n",
    "        i=0\n",
    "        for input_data, y_true in genfun:\n",
    "            \n",
    "            y_pred = model.predict(input_data, batch_size=len(y_true))\n",
    "            if issubclass(type(generator), inputs.list_generator.ListBasicGenerator):\n",
    "                list_counts = input_data['list_counts']\n",
    "                for k, eval_func in eval_metrics.items():\n",
    "                    for lc_idx in range(len(list_counts)-1):\n",
    "                        pre = list_counts[lc_idx]\n",
    "                        suf = list_counts[lc_idx+1]\n",
    "                        res[k] += eval_func(y_true = y_true[pre:suf], y_pred = y_pred[pre:suf])\n",
    "                num_valid += len(list_counts) - 1\n",
    "            else:\n",
    "                for k, eval_func in eval_metrics.items():\n",
    "                    res[k] += eval_func(y_true = y_true, y_pred = y_pred)\n",
    "                num_valid += 1\n",
    "        i=i+1\n",
    "        generator.reset()\n",
    "        print('Iter:%d\\t%s' % (i_e, '\\t'.join(['%s=%f'%(k,v/num_valid) for k, v in res.items()])), end='\\n')\n",
    "        sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'inputs.point_generator.DRMM_PointGenerator'>\n",
      "[08-09-2018 10:10:36]\t[Train:train] Epoch 1/1\n",
      "15138/30000 [==============>...............] - ETA: 37s - loss: 0.6139"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1de3514513fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             ) #callbacks=[eval_map])\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iter:%d\\tloss=%.6f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2228\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2229\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_iters=1000\n",
    "save_weights_iters=50\n",
    "test_weights_iters=50\n",
    "display_interval=30000\n",
    "\n",
    "for i_e in range(100):\n",
    "    for tag, generator in train_gen.items():\n",
    "        genfun = generator.get_batch_generator()\n",
    "        print(type(generator))\n",
    "        print('[%s]\\t[Train:%s] ' % (time.strftime('%m-%d-%Y %H:%M:%S', time.localtime(time.time())), tag), end='')\n",
    "        history = model.fit_generator(\n",
    "                genfun,\n",
    "                steps_per_epoch = display_interval,\n",
    "                #steps_per_epoch=1000,\n",
    "                epochs = 1,\n",
    "                shuffle=False,\n",
    "                verbose = 1\n",
    "            ) #callbacks=[eval_map])\n",
    "        print('Iter:%d\\tloss=%.6f' % (i_e, history.history['loss'][0]), end='\\n')\n",
    "    if((i_e+1)%test_weights_iters==0):\n",
    "        evaluate()\n",
    "        \n",
    "  \n",
    "    if (i_e+1) % save_weights_iters == 0:\n",
    "        model.save_weights(weights_file % (i_e+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08-09-2018 01:43:18]\t[Eval:valid] [08-09-2018 01:43:18]\t[Eval:test] "
     ]
    }
   ],
   "source": [
    "for tag, generator in eval_gen.items():\n",
    "\n",
    "        genfun = generator.get_batch_generator()\n",
    "        print('[%s]\\t[Eval:%s] ' % (time.strftime('%m-%d-%Y %H:%M:%S', time.localtime(time.time())), tag), end='')\n",
    "        res = dict([[k,0.] for k in eval_metrics.keys()])\n",
    "        num_valid = 0\n",
    "        i\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    " for input_data, y_true in genfun:\n",
    "\n",
    "            y_pred = model.predict(input_data, batch_size=len(y_true))\n",
    "            if issubclass(type(generator), inputs.list_generator.ListBasicGenerator):\n",
    "                list_counts = input_data['list_counts']\n",
    "                for k, eval_func in eval_metrics.items():\n",
    "                    for lc_idx in range(len(list_counts)-1):\n",
    "                        pre = list_counts[lc_idx]\n",
    "                        suf = list_counts[lc_idx+1]\n",
    "                        res[k] += eval_func(y_true = y_true[pre:suf], y_pred = y_pred[pre:suf])\n",
    "                num_valid += len(list_counts) - 1\n",
    "            else:\n",
    "                for k, eval_func in eval_metrics.items():\n",
    "                    res[k] += eval_func(y_true = y_true, y_pred = y_pred)\n",
    "                num_valid += 1\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=next(genfun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=a[0]\n",
    "y_true=a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(input_data, batch_size=len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1]], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0], dtype=int32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'net_name': 'ANMM',\n",
       " 'global': {'model_type': 'PY',\n",
       "  'weights_file': 'weights/anmm_classify.weights',\n",
       "  'save_weights_iters': 5,\n",
       "  'num_iters': 100,\n",
       "  'display_interval': 5000,\n",
       "  'test_weights_iters': 5,\n",
       "  'optimizer': 'adam',\n",
       "  'learning_rate': 0.0001},\n",
       " 'inputs': {'share': {'text1_corpus': 'corpus_preprocessed.txt',\n",
       "   'text2_corpus': 'corpus_preprocessed.txt',\n",
       "   'embed_path': 'embed_glove_d300_norm',\n",
       "   'use_dpool': False,\n",
       "   'embed_size': 300,\n",
       "   'vocab_size': 28162,\n",
       "   'train_embed': False,\n",
       "   'target_mode': 'classification',\n",
       "   'bin_num': 100,\n",
       "   'text1_maxlen': 3,\n",
       "   'text2_maxlen': 100,\n",
       "   'embed': array([[ 0.036746,  0.103572, -0.007838, ...,  0.108598, -0.063425,\n",
       "            0.045992],\n",
       "          [ 0.022771, -0.013996, -0.033078, ...,  0.045675,  0.047962,\n",
       "           -0.028272],\n",
       "          [-0.027226, -0.01897 ,  0.042221, ..., -0.056826,  0.040489,\n",
       "            0.03461 ],\n",
       "          ...,\n",
       "          [ 0.101635,  0.109067,  0.029523, ..., -0.049203,  0.085788,\n",
       "           -0.036978],\n",
       "          [ 0.017096, -0.002005,  0.139227, ...,  0.046317, -0.02477 ,\n",
       "           -0.08832 ],\n",
       "          [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "            0.      ]], dtype=float32)},\n",
       "  'train': {'input_type': 'DRMM_PointGenerator',\n",
       "   'phase': 'TRAIN',\n",
       "   'use_iter': False,\n",
       "   'query_per_iter': 20000,\n",
       "   'batch_per_iter': 5000,\n",
       "   'batch_size': 20,\n",
       "   'relation_file': 'relation_train.txt',\n",
       "   'hist_feats_file': 'relation.train.binsum-100.txt'},\n",
       "  'valid': {'input_type': 'DRMM_PointGenerator',\n",
       "   'phase': 'EVAL',\n",
       "   'batch_size': 10,\n",
       "   'relation_file': 'relation_valid.txt',\n",
       "   'hist_feats_file': 'relation.valid.binsum-100.txt'},\n",
       "  'test': {'input_type': 'DRMM_PointGenerator',\n",
       "   'phase': 'EVAL',\n",
       "   'batch_size': 10,\n",
       "   'relation_file': 'relation_test.txt',\n",
       "   'hist_feats_file': 'relation.test.binsum-100.txt'},\n",
       "  'predict': {'input_type': 'DRMM_PointGenerator',\n",
       "   'phase': 'PREDICT',\n",
       "   'batch_size': 1,\n",
       "   'relation_file': 'relation_test.txt',\n",
       "   'hist_feats_file': 'relation.test.binsum-100.txt'}},\n",
       " 'outputs': {'predict': {'save_format': 'TREC',\n",
       "   'save_path': 'predict.test.anmm_classify.txt'}},\n",
       " 'model': {'model_path': 'models/',\n",
       "  'model_py': 'anmm.ANMM',\n",
       "  'setting': {'num_layers': 2,\n",
       "   'hidden_sizes': [50, 1],\n",
       "   'dropout_rate': 0.5,\n",
       "   'text1_corpus': 'corpus_preprocessed.txt',\n",
       "   'text2_corpus': 'corpus_preprocessed.txt',\n",
       "   'embed_path': 'embed_glove_d300_norm',\n",
       "   'use_dpool': False,\n",
       "   'embed_size': 300,\n",
       "   'vocab_size': 28162,\n",
       "   'train_embed': False,\n",
       "   'target_mode': 'classification',\n",
       "   'bin_num': 100,\n",
       "   'text1_maxlen': 3,\n",
       "   'text2_maxlen': 100,\n",
       "   'embed': array([[ 0.036746,  0.103572, -0.007838, ...,  0.108598, -0.063425,\n",
       "            0.045992],\n",
       "          [ 0.022771, -0.013996, -0.033078, ...,  0.045675,  0.047962,\n",
       "           -0.028272],\n",
       "          [-0.027226, -0.01897 ,  0.042221, ..., -0.056826,  0.040489,\n",
       "            0.03461 ],\n",
       "          ...,\n",
       "          [ 0.101635,  0.109067,  0.029523, ..., -0.049203,  0.085788,\n",
       "           -0.036978],\n",
       "          [ 0.017096, -0.002005,  0.139227, ...,  0.046317, -0.02477 ,\n",
       "           -0.08832 ],\n",
       "          [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "            0.      ]], dtype=float32)}},\n",
       " 'losses': [{'object_name': 'categorical_crossentropy', 'object_params': {}}],\n",
       " 'metrics': ['accuracy']}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_items([('train', <inputs.pair_generator.DRMM_PairGenerator object at 0x7fcf8dd77208>)])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gen.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
